{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/20000, thread: 0, score: -21.0, average: -21.00 SAVING\n",
      "episode: 1/20000, thread: 0, score: -21.0, average: -21.00 SAVING\n",
      "episode: 2/20000, thread: 0, score: -21.0, average: -21.00 SAVING\n",
      "episode: 3/20000, thread: 0, score: -21.0, average: -21.00 SAVING\n",
      "episode: 4/20000, thread: 0, score: -21.0, average: -21.00 SAVING\n",
      "episode: 5/20000, thread: 0, score: -20.0, average: -20.83 SAVING\n",
      "episode: 6/20000, thread: 0, score: -21.0, average: -20.86 \n",
      "episode: 7/20000, thread: 0, score: -20.0, average: -20.75 SAVING\n",
      "episode: 8/20000, thread: 0, score: -20.0, average: -20.67 SAVING\n",
      "episode: 9/20000, thread: 0, score: -20.0, average: -20.60 SAVING\n",
      "episode: 10/20000, thread: 0, score: -21.0, average: -20.64 \n"
     ]
    }
   ],
   "source": [
    "# Tutorial by www.pylessons.com\n",
    "# Tutorial written for - Tensorflow 2.3.1\n",
    "\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "import threading\n",
    "from threading import Thread, Lock\n",
    "import time\n",
    "import tensorflow_probability as tfp\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class ActorCritic(tf.keras.Model):\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "  def __init__(\n",
    "      self, \n",
    "      num_actions: int, \n",
    "      num_hidden_units: int):\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_actions = num_actions\n",
    "    \n",
    "    self.conv_1 = tf.keras.layers.Conv2D(16, 8, 4, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.conv_2 = tf.keras.layers.Conv2D(32, 4, 2, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.conv_3 = tf.keras.layers.Conv2D(32, 3, 1, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    \n",
    "    self.lstm = tf.keras.layers.LSTM(128, return_sequences=True, return_state=True, kernel_regularizer='l2')\n",
    "    \n",
    "    self.common = tf.keras.layers.Dense(num_hidden_units, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.actor = tf.keras.layers.Dense(num_actions, kernel_regularizer='l2')\n",
    "    self.critic = tf.keras.layers.Dense(1, kernel_regularizer='l2')\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super().get_config().copy()\n",
    "    config.update({\n",
    "        'num_actions': self.num_actions,\n",
    "        'num_hidden_units': self.num_hidden_units\n",
    "    })\n",
    "    return config\n",
    "    \n",
    "  def call(self, inputs: tf.Tensor, memory_state: tf.Tensor, carry_state: tf.Tensor, training) -> Tuple[tf.Tensor, tf.Tensor, \n",
    "                                                                                                        tf.Tensor, tf.Tensor]:\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    conv_1 = self.conv_1(inputs)\n",
    "    conv_2 = self.conv_2(conv_1)\n",
    "    conv_3 = self.conv_3(conv_2)\n",
    "    conv_3_reshaped = tf.keras.layers.Reshape((4*4,32))(conv_3)\n",
    "    \n",
    "    initial_state = (memory_state, carry_state)\n",
    "    lstm_output, final_memory_state, final_carry_state  = self.lstm(conv_3_reshaped, initial_state=initial_state, \n",
    "                                                                    training=training)\n",
    "    X_input = tf.keras.layers.Flatten()(lstm_output)\n",
    "    x = self.common(X_input)\n",
    "    \n",
    "    return self.actor(x), self.critic(x), memory_state, carry_state\n",
    "\n",
    "\n",
    "def safe_log(x):\n",
    "  \"\"\"Computes a safe logarithm which returns 0 if x is zero.\"\"\"\n",
    "  return tf.where(\n",
    "      tf.math.equal(x, 0),\n",
    "      tf.zeros_like(x),\n",
    "      tf.math.log(tf.math.maximum(1e-12, x)))\n",
    "\n",
    "\n",
    "def take_vector_elements(vectors, indices):\n",
    "    \"\"\"\n",
    "    For a batch of vectors, take a single vector component\n",
    "    out of each vector.\n",
    "    Args:\n",
    "      vectors: a [batch x dims] Tensor.\n",
    "      indices: an int32 Tensor with `batch` entries.\n",
    "    Returns:\n",
    "      A Tensor with `batch` entries, one for each vector.\n",
    "    \"\"\"\n",
    "    return tf.gather_nd(vectors, tf.stack([tf.range(tf.shape(vectors)[0]), indices], axis=1))\n",
    "\n",
    "\n",
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "sparse_ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "\n",
    "class A3CAgent:\n",
    "    # Actor-Critic Main Optimization Algorithm\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES, self.episode, self.max_average = 20000, 0, -21.0 # specific for pong\n",
    "        \n",
    "        memory_size = 10000\n",
    "        self.memory = []\n",
    "        self.lock = Lock()\n",
    "        self.lr = 0.0001\n",
    "\n",
    "        num_hidden_units = 512\n",
    "        \n",
    "        self.batch_size = 64\n",
    "\n",
    "        # Instantiate plot memory\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Save_Path = 'Models'\n",
    "        \n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.path = '{}_A3C_{}'.format(self.env_name, self.lr)\n",
    "        self.model_name = os.path.join(self.Save_Path, self.path)\n",
    "\n",
    "        # Create Actor-Critic network model\n",
    "        self.model = ActorCritic(self.action_size, num_hidden_units)\n",
    "        \n",
    "        self.learning_rate = 0.0001\n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.lr)\n",
    "\n",
    "    def remember(self, state, action, policy, reward, done, memory_state, carry_state):\n",
    "        experience = state, action, policy, reward, done, memory_state, carry_state\n",
    "        self.memory.append((experience))\n",
    "        \n",
    "    def act(self, state, memory_state, carry_state):\n",
    "        prediction = self.model(state, memory_state, carry_state, training=False)\n",
    "        \n",
    "        dist = tfd.Categorical(logits=prediction[0])\n",
    "        action = int(dist.sample()[0])\n",
    "        policy = prediction[0]\n",
    "        \n",
    "        memory_state = prediction[2]\n",
    "        carry_state = prediction[3]\n",
    "        \n",
    "        return action, policy, memory_state, carry_state\n",
    "\n",
    "    def update(self, states, actions, agent_policies, rewards, dones, memory_states, carry_states):\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        agent_policies = tf.convert_to_tensor(agent_policies, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(dones, dtype=tf.bool)\n",
    "        memory_states = tf.convert_to_tensor(memory_states, dtype=tf.float32)\n",
    "        carry_states = tf.convert_to_tensor(carry_states, dtype=tf.float32)\n",
    "\n",
    "        batch_size = states.shape[0]\n",
    "\n",
    "        online_variables = self.model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(online_variables)\n",
    "\n",
    "            learner_policies = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "            learner_values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "            memory_state = tf.expand_dims(memory_states[0], 0)\n",
    "            carry_state = tf.expand_dims(carry_states[0], 0)\n",
    "            for i in tf.range(0, batch_size):\n",
    "                learner_output = self.model(tf.expand_dims(states[i,:,:,:], 0), memory_state, carry_state,\n",
    "                                                           training=True)\n",
    "                learner_policy = learner_output[0]\n",
    "                learner_policy = tf.squeeze(learner_policy)\n",
    "                learner_policies = learner_policies.write(i, learner_policy)\n",
    "\n",
    "                learner_value = learner_output[1]\n",
    "                learner_value = tf.squeeze(learner_value)\n",
    "                learner_values = learner_values.write(i, learner_value)\n",
    "\n",
    "                memory_state = learner_output[2]\n",
    "                carry_state = learner_output[3]\n",
    "\n",
    "            learner_policies = learner_policies.stack()\n",
    "            learner_values = learner_values.stack()\n",
    "\n",
    "            learner_logits = tf.nn.softmax(learner_policies[:-1])\n",
    "            agent_logits = tf.nn.softmax(agent_policies[:-1])\n",
    "\n",
    "            actions = actions[:-1]\n",
    "            rewards = rewards[1:]\n",
    "            dones = dones[1:]\n",
    "\n",
    "            bootstrap_value = learner_values[-1]\n",
    "            learner_values = learner_values[:-1]\n",
    "\n",
    "            discounting = 0.99\n",
    "            discounts = tf.cast(~dones, tf.float32) * discounting\n",
    "\n",
    "            target_action_probs = take_vector_elements(learner_logits, actions)\n",
    "            target_action_log_probs = tf.math.log(target_action_probs)\n",
    "\n",
    "            behaviour_action_probs = take_vector_elements(agent_logits, actions)\n",
    "            behaviour_action_log_probs = tf.math.log(behaviour_action_probs)\n",
    "\n",
    "            lambda_ = 1.0\n",
    "\n",
    "            log_rhos = target_action_log_probs - behaviour_action_log_probs\n",
    "\n",
    "            log_rhos = tf.convert_to_tensor(log_rhos, dtype=tf.float32)\n",
    "            discounts = tf.convert_to_tensor(discounts, dtype=tf.float32)\n",
    "            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "            values = tf.convert_to_tensor(learner_values, dtype=tf.float32)\n",
    "            bootstrap_value = tf.convert_to_tensor(bootstrap_value, dtype=tf.float32)\n",
    "\n",
    "            clip_rho_threshold = tf.convert_to_tensor(1.0, dtype=tf.float32)\n",
    "            clip_pg_rho_threshold = tf.convert_to_tensor(1.0, dtype=tf.float32)\n",
    "\n",
    "            rhos = tf.math.exp(log_rhos)\n",
    "\n",
    "            clipped_rhos = tf.minimum(clip_rho_threshold, rhos, name='clipped_rhos')\n",
    "\n",
    "            cs = tf.minimum(1.0, rhos, name='cs')\n",
    "            cs *= tf.convert_to_tensor(lambda_, dtype=tf.float32)\n",
    "\n",
    "            values_t_plus_1 = tf.concat([values[1:], tf.expand_dims(bootstrap_value, 0)], axis=0)\n",
    "            deltas = clipped_rhos * (rewards + discounts * values_t_plus_1 - values)\n",
    "\n",
    "            acc = tf.zeros_like(bootstrap_value)\n",
    "            vs_minus_v_xs = []\n",
    "            for i in range(int(discounts.shape[0]) - 1, -1, -1):\n",
    "                discount, c, delta = discounts[i], cs[i], deltas[i]\n",
    "                acc = delta + discount * c * acc\n",
    "                vs_minus_v_xs.append(acc)  \n",
    "\n",
    "            vs_minus_v_xs = vs_minus_v_xs[::-1]\n",
    "\n",
    "            vs = tf.add(vs_minus_v_xs, values, name='vs')\n",
    "            vs_t_plus_1 = tf.concat([vs[1:], tf.expand_dims(bootstrap_value, 0)], axis=0)\n",
    "            clipped_pg_rhos = tf.minimum(clip_pg_rho_threshold, rhos, name='clipped_pg_rhos')\n",
    "\n",
    "            pg_advantages = (clipped_pg_rhos * (rewards + discounts * vs_t_plus_1 - values))\n",
    "\n",
    "            vs = tf.stop_gradient(vs)\n",
    "            pg_advantages = tf.stop_gradient(pg_advantages)\n",
    "\n",
    "            actor_loss = -tf.reduce_mean(target_action_log_probs * pg_advantages)\n",
    "\n",
    "            baseline_cost = 0.5\n",
    "            v_error = values - vs\n",
    "            critic_loss = baseline_cost * 0.5 * tf.reduce_mean(tf.square(v_error))\n",
    "\n",
    "            total_loss = actor_loss + critic_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "    \n",
    "    def replay(self):\n",
    "        memory_len = len(self.memory)\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            start_index = random.randint(0, memory_len - self.batch_size)\n",
    "            minibatch = self.memory[start_index:start_index+self.batch_size]\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        states = np.zeros((self.batch_size, 64, 64, 3), dtype=np.float32)\n",
    "        actions = np.zeros(self.batch_size, dtype=np.int32)\n",
    "        policies = np.zeros((self.batch_size, self.action_size), dtype=np.float32)\n",
    "        rewards = np.zeros(self.batch_size, dtype=np.float32)\n",
    "        dones = np.zeros(self.batch_size, dtype=np.bool)\n",
    "        memory_states = np.zeros((self.batch_size, 128), dtype=np.float32)\n",
    "        carry_states = np.zeros((self.batch_size, 128), dtype=np.float32)\n",
    "\n",
    "        for i in range(len(minibatch)):\n",
    "            states[i] = minibatch[i][0]\n",
    "            actions[i] = minibatch[i][1]\n",
    "            policies[i] = minibatch[i][2]\n",
    "            rewards[i] = minibatch[i][3]\n",
    "            dones[i] = minibatch[i][4]\n",
    "            memory_states[i] = minibatch[i][5]\n",
    "            carry_states[i] = minibatch[i][6]\n",
    "\n",
    "        self.update(states, actions, policies, rewards, dones, memory_states, carry_states)\n",
    "        \n",
    "    def load(self, model_name):\n",
    "        self.ActorCritic = load_model(model_name, compile=False)\n",
    "        #self.Critic = load_model(Critic_name, compile=False)\n",
    "\n",
    "    def save(self):\n",
    "        self.ActorCritic.save(self.model_name)\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            pylab.plot(self.episodes, self.scores, 'b')\n",
    "            pylab.plot(self.episodes, self.average, 'r')\n",
    "            pylab.ylabel('Score', fontsize=18)\n",
    "            pylab.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                pylab.savefig(self.path + \".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        return self.average[-1]\n",
    "    \n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(self.model_name + str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def reset(self, env):\n",
    "        state = env.reset()\n",
    "            \n",
    "        return state\n",
    "    \n",
    "    def step(self, action, env):\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def train(self, n_threads):\n",
    "        self.env.close()\n",
    "        # Instantiate one environment per thread\n",
    "        envs = [gym.make(self.env_name) for i in range(n_threads)]\n",
    "\n",
    "        # Create threads\n",
    "        threads = [threading.Thread(\n",
    "                target=self.train_threading,\n",
    "                daemon=True,\n",
    "                args=(self,\n",
    "                    envs[i],\n",
    "                    i)) for i in range(n_threads)]\n",
    "\n",
    "        for t in threads:\n",
    "            time.sleep(2)\n",
    "            t.start()\n",
    "            \n",
    "        for t in threads:\n",
    "            time.sleep(10)\n",
    "            t.join()\n",
    "    \n",
    "    def render(self, obs):\n",
    "        #obs = cv2.cvtColor(obs, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imshow('obs', obs)\n",
    "        cv2.waitKey(1)\n",
    "    \n",
    "    def train_threading(self, agent, env, thread):\n",
    "        max_average = 15.0\n",
    "        total_step = 0\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset(env)\n",
    "            #print(\"state.shape: \", state.shape)\n",
    "            state = state[35:195:2, ::2,:]\n",
    "            state = cv2.resize(state, (64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "            state = np.expand_dims(state, 0) / 255.0\n",
    "            \n",
    "            done = False\n",
    "            score = 0\n",
    "            SAVING = ''\n",
    "            \n",
    "            memory_state = tf.zeros([1,128], dtype=tf.dtypes.float32)\n",
    "            carry_state = tf.zeros([1,128], dtype=tf.dtypes.float32)\n",
    "            while not done:\n",
    "                #self.env.render()\n",
    "                #self.render(state[0])\n",
    "                \n",
    "                action, policy, memory_state, carry_state = self.act(state, memory_state, carry_state)\n",
    "                \n",
    "                next_state, reward, done, _ = self.step(action, env)\n",
    "                next_state = next_state[35:195:2, ::2,:]\n",
    "                next_state = cv2.resize(next_state, (64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "                next_state = np.expand_dims(next_state, 0) / 255.0\n",
    "                \n",
    "                self.remember(state, action, policy, reward / 20.0, done, memory_state, carry_state)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "\n",
    "                if done:           \n",
    "                    break\n",
    "                \n",
    "                self.lock.acquire()\n",
    "                if total_step % 100 == 0:\n",
    "                    # train model\n",
    "                    self.replay()\n",
    "                self.lock.release()\n",
    "                    \n",
    "                total_step += 1\n",
    "                \n",
    "            # Update episode count\n",
    "            with self.lock:\n",
    "                average = self.PlotModel(score, self.episode)\n",
    "                # saving best models\n",
    "                if average >= self.max_average:\n",
    "                    self.max_average = average\n",
    "                    #self.save()\n",
    "                    SAVING = \"SAVING\"\n",
    "                else:\n",
    "                    SAVING = \"\"\n",
    "\n",
    "                print(\"episode: {}/{}, thread: {}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, thread, score, average, SAVING))\n",
    "                if(self.episode < self.EPISODES):\n",
    "                    self.episode += 1\n",
    "                 \n",
    "    def test(self, Actor_name, Critic_name):\n",
    "        self.load(Actor_name, Critic_name)\n",
    "        for e in range(100):\n",
    "            state = self.reset(self.env)\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.Actor.predict(state))\n",
    "                state, reward, done, _ = self.step(action, self.env, state)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'PongDeterministic-v4'\n",
    "    #env_name = 'Pong-v0'\n",
    "    agent = A3CAgent(env_name)\n",
    "    \n",
    "    #agent.run() # use as A2C\n",
    "    agent.train(n_threads=1) # use as A3C\n",
    "    #agent.test('Models/Pong-v0_A3C_2.5e-05_Actor.h5', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysc2_env",
   "language": "python",
   "name": "pysc2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
