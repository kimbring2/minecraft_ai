{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-17 04:42:15.665275: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "/home/kimbring2/.local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2021-10-17 04:42:17.659360: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-17 04:42:17.660030: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-10-17 04:42:17.682747: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-10-17 04:42:17.682772: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: kimbring2-GF75-Thin-10UEK\n",
      "2021-10-17 04:42:17.682776: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: kimbring2-GF75-Thin-10UEK\n",
      "2021-10-17 04:42:17.682837: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.91.3\n",
      "2021-10-17 04:42:17.682854: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
      "2021-10-17 04:42:17.682859: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.91.3\n",
      "2021-10-17 04:42:17.683408: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import glob\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "import gym\n",
    "import minerl\n",
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "#            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "workspace_path = '/home/kimbring2/minecraft_ai'\n",
    "\n",
    "writer = tf.summary.create_file_writer(workspace_path + \"/tensorboard\")\n",
    "\n",
    "env = gym.make('MineRLNavigateDense-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "  def __init__(\n",
    "      self, \n",
    "      num_actions: int, \n",
    "      num_hidden_units: int):\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_actions = num_actions\n",
    "    \n",
    "    self.conv_1 = layers.Conv2D(16, 8, 4, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.conv_2 = layers.Conv2D(32, 4, 2, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.conv_3 = layers.Conv2D(32, 3, 1, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    \n",
    "    self.lstm = layers.LSTM(128, return_sequences=True, return_state=True, kernel_regularizer='l2')\n",
    "    \n",
    "    self.common = layers.Dense(num_hidden_units, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.actor = layers.Dense(num_actions, kernel_regularizer='l2')\n",
    "    self.critic = layers.Dense(1, kernel_regularizer='l2')\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super().get_config().copy()\n",
    "    config.update({\n",
    "        'num_actions': self.num_actions,\n",
    "        'num_hidden_units': self.num_hidden_units\n",
    "    })\n",
    "    return config\n",
    "    \n",
    "  def call(self, inputs: tf.Tensor, memory_state: tf.Tensor, carry_state: tf.Tensor, training) -> Tuple[tf.Tensor, tf.Tensor, \n",
    "                                                                                                        tf.Tensor, tf.Tensor]:\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    conv_1 = self.conv_1(inputs)\n",
    "    conv_2 = self.conv_2(conv_1)\n",
    "    conv_3 = self.conv_3(conv_2)\n",
    "    #print(\"conv_3.shape: \", conv_3.shape)\n",
    "    conv_3_reshaped = layers.Reshape((4*4,32))(conv_3)\n",
    "    \n",
    "    initial_state = (memory_state, carry_state)\n",
    "    #print(\"initial_state: \", initial_state)\n",
    "    lstm_output, final_memory_state, final_carry_state  = self.lstm(conv_3_reshaped, initial_state=initial_state, \n",
    "                                                                    training=training)\n",
    "    \n",
    "    X_input = layers.Flatten()(lstm_output)\n",
    "    x = self.common(X_input)\n",
    "    \n",
    "    return tf.keras.layers.Softmax()(self.actor(x)), self.critic(x), final_memory_state, final_carry_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 20\n",
    "num_hidden_units = 512\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(reward, dones):\n",
    "    # Compute the gamma-discounted rewards over an episode\n",
    "    gamma = 0.99    # discount rate\n",
    "    running_add = 0\n",
    "    discounted_r = np.zeros_like(reward)\n",
    "    for i in reversed(range(0, len(reward))):\n",
    "        running_add = running_add * gamma * (1 - dones[i]) + reward[i]\n",
    "        discounted_r[i] = running_add\n",
    "\n",
    "    if np.std(discounted_r) != 0:\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= np.std(discounted_r) # divide by standard deviation\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "def take_vector_elements(vectors, indices):\n",
    "    return tf.gather_nd(vectors, tf.stack([tf.range(tf.shape(vectors)[0]), indices], axis=1))\n",
    "\n",
    "\n",
    "def render(obs):\n",
    "    obs = cv2.cvtColor(obs, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('obs', obs)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "\n",
    "@tf.function\n",
    "def get_loss(input_array, action_list, memory_state, carry_state, discounted_r):\n",
    "    batch_size = input_array.shape[0]\n",
    "    \n",
    "    action_logits = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    for i in tf.range(0, batch_size):\n",
    "        model_input = tf.expand_dims(input_array[i,:,:,:], 0)\n",
    "        prediction = model(model_input, memory_state, carry_state, training=False)\n",
    "        action_logit = prediction[0]\n",
    "        value = prediction[1]\n",
    "        memory_state = prediction[2]\n",
    "        carry_state = prediction[3]\n",
    "\n",
    "        action_logits = action_logits.write(i, action_logit[0])\n",
    "        values = values.write(i, tf.squeeze(value))\n",
    "            \n",
    "    action_logits = action_logits.stack()\n",
    "    values = values.stack()\n",
    "    #tf.print(\"values: \", values)\n",
    "\n",
    "    action_logits_selected = take_vector_elements(action_logits, action_list)\n",
    "\n",
    "    #tf.print(\"discounted_r: \", discounted_r)\n",
    "    #tf.print(\"values: \", values)\n",
    "    discounted_r = tf.cast(discounted_r, 'float32')\n",
    "    advantages = discounted_r - values\n",
    "    #tf.print(\"advantages: \", advantages)\n",
    "    #tf.print(\"advantages test: \", discounted_r_array - values)\n",
    "            \n",
    "    action_logits_selected_probs = tf.math.log(action_logits_selected)\n",
    "        \n",
    "    #tf.print(\"action_logits_selected_probs: \", action_logits_selected_probs)\n",
    "    #tf.print(\"advantages: \", advantages)\n",
    "    actor_loss = -tf.math.reduce_mean(action_logits_selected_probs * tf.stop_gradient(advantages)) \n",
    "    actor_loss = tf.cast(actor_loss, 'float32')\n",
    "            \n",
    "    critic_loss = mse_loss(values, discounted_r)\n",
    "    critic_loss = tf.cast(critic_loss, 'float32')\n",
    "        \n",
    "    total_loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "    return total_loss, memory_state, carry_state\n",
    "\n",
    "\n",
    "def reinforcement_replay(input_list, action_list, memory_state, carry_state, reward_list, done_list):\n",
    "    input_array = tf.concat(input_list, 0)\n",
    "    memory_state = tf.concat(memory_state, 0)\n",
    "    carry_state = tf.concat(carry_state, 0)\n",
    "    \n",
    "    #print(\"action_list: \", action_list)\n",
    "    action_array = tf.concat(action_list, 0)\n",
    "    #print(\"action_array: \", action_array)\n",
    "\n",
    "    discounted_r_array = discount_rewards(reward_list, done_list)\n",
    "    discounted_r = tf.concat(discounted_r_array, 0)\n",
    "    with tf.GradientTape() as tape:\n",
    "        total_loss, memory_state, carry_state = get_loss(input_array, action_array, memory_state, \n",
    "                                                         carry_state, discounted_r)\n",
    "        print(\"total_loss: \", total_loss)\n",
    "        \n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return total_loss, memory_state, carry_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reinforcement_train(training_episode):\n",
    "    total_reward, done, SAVING = 0, False, ''\n",
    "    obs = env.reset()\n",
    "    \n",
    "    input_list, action_list, reward_list, done_list = [], [], [], []\n",
    "    \n",
    "    memory_state = tf.zeros([1,128], dtype=np.float32)\n",
    "    carry_state = tf.zeros([1,128], dtype=np.float32)\n",
    "    \n",
    "    initial_memory_state = memory_state\n",
    "    initial_carry_state = carry_state\n",
    "    while True:\n",
    "        render(obs['pov'])\n",
    "        \n",
    "        pov_array = obs['pov'] / 255.0\n",
    "        \n",
    "        compassAngle_array = obs['compass']['angle'] / 360.0\n",
    "        compassAngle_array = np.ones((64,64,1)) * compassAngle_array\n",
    "        \n",
    "        input_array = np.concatenate((pov_array, compassAngle_array), 2)\n",
    "        input_array = np.expand_dims(input_array, 0)\n",
    "        \n",
    "        prediction = model(input_array, memory_state, carry_state, training=False)\n",
    "        act_pi = prediction[0]\n",
    "        memory_state = prediction[2]\n",
    "        carry_state = prediction[3]\n",
    "        \n",
    "        action_dist = tfd.Categorical(probs=act_pi)\n",
    "        action_index = int(action_dist.sample()[0])\n",
    "        \n",
    "        action = env.action_space.noop()\n",
    "        if (action_index == 0):\n",
    "            action['camera'] = [0, -5]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 1):\n",
    "            action['camera'] = [0, -5]\n",
    "            action['attack'] = 1\n",
    "        elif (action_index == 2):\n",
    "            action['camera'] = [0, 5]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 3):\n",
    "            action['camera'] = [0, 5]\n",
    "            action['attack'] = 1\n",
    "        elif (action_index == 4):\n",
    "            action['camera'] = [-5, 0]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 5):\n",
    "            action['camera'] = [-5, 0]\n",
    "            action['attack'] = 1\n",
    "        elif (action_index == 6):\n",
    "            action['camera'] = [5, 0]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 7):\n",
    "            action['camera'] = [5, 0]\n",
    "            \n",
    "        elif (action_index == 8):\n",
    "            action['forward'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 9):\n",
    "            action['forward'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 10):\n",
    "            action['jump'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 11):\n",
    "            action['jump'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 12):\n",
    "            action['back'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 13):\n",
    "            action['back'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 14):\n",
    "            action['left'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 15):\n",
    "            action['left'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 16):\n",
    "            action['right'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 17):\n",
    "            action['right'] = 1\n",
    "            action['attack'] = 1 \n",
    "            \n",
    "        elif (action_index == 18):\n",
    "            action['sneak'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 19):\n",
    "            action['sneak'] = 1\n",
    "            action['attack'] = 1 \n",
    "        \n",
    "        obs_1, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        input_list.append(input_array)\n",
    "        action_list.append(action_index)\n",
    "        reward_list.append(reward)\n",
    "        done_list.append(done)\n",
    "        \n",
    "        obs = obs_1\n",
    "        if done:\n",
    "            print(\"total_reward: \", total_reward)\n",
    "            \n",
    "            with writer.as_default():\n",
    "                tf.summary.scalar(\"total_reward\", total_reward, step=training_episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            break\n",
    "        \n",
    "        if len(input_list) == 64:   \n",
    "            total_loss, memory_state, carry_state = reinforcement_replay(input_list, action_list,\n",
    "                                                                         initial_memory_state, \n",
    "                                                                         initial_carry_state,\n",
    "                                                                         reward_list, done_list)\n",
    "\n",
    "            input_list, action_list, reward_list, done_list = [], [], [], []\n",
    "            \n",
    "            initial_memory_state = memory_state\n",
    "            initial_carry_state = carry_state\n",
    "            \n",
    "            #print(\"total_loss: \", total_loss)\n",
    "            #print(\"\")\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "\n",
    "            \n",
    "max_episodes = 200000\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "  for i in t:\n",
    "    #print(\"i: \", i)\n",
    "    reinforcement_train(i)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        model.save_weights(workspace_path + '/model/' + str(i))\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import gym\n",
    "import minerl\n",
    "\n",
    "model.load_weights(workspace_path + \"/model/supervised_model_12000\")\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('MineRLTreechop-v0')\n",
    "\n",
    "seed = 980\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "reward_sum = 0\n",
    "for i_episode in range(0, 10000):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    inventory_channel = np.zeros((64,64,1))\n",
    "    if 'inventory' in observation:\n",
    "        region_max_height = observation['pov'].shape[0]\n",
    "        region_max_width = observation['pov'].shape[1]\n",
    "        rs = 8\n",
    "        if min(region_max_height, region_max_width) < rs:\n",
    "            raise ValueError(\"'region_size' is too large.\")\n",
    "            \n",
    "        num_element_width = region_max_width // rs\n",
    "\n",
    "        inventory_channel = np.zeros(shape=list(observation['pov'].shape[:-1]) + [1], \n",
    "                                     dtype=observation['pov'].dtype)\n",
    "        #print(\"state['inventory'].keys(): \" + str(state['inventory'].keys()))\n",
    "        for key_idx, key in enumerate(observation['inventory'].keys()):\n",
    "            #print(\"key.shape : \" + str(key))\n",
    "            #print(\"state['inventory'][key][i] : \" + str(state['inventory'][key][i]))\n",
    "            item_scaled = np.clip(1 - 1 / (observation['inventory'][key] + 1),  # Inversed\n",
    "                                  0, 1)\n",
    "            #print(\"item_scaled : \" + str(item_scaled))\n",
    "            item_channel = np.ones(shape=[rs, rs, 1], dtype=observation['pov'].dtype) * item_scaled\n",
    "            width_low = (key_idx % num_element_width) * rs\n",
    "            height_low = (key_idx // num_element_width) * rs\n",
    "\n",
    "            if height_low + rs > region_max_height:\n",
    "                raise ValueError(\"Too many elements on 'inventory'. Please decrease 'region_size' of each component.\")\n",
    "\n",
    "            inventory_channel[height_low:(height_low + rs), width_low:(width_low + rs), :] = item_channel\n",
    "\n",
    "    state = np.concatenate((observation['pov'] / 255.0, inventory_channel), axis=2)\n",
    "    state = tf.constant(state, dtype=tf.float32)\n",
    "    \n",
    "    memory_state = tf.zeros([1,128], dtype=np.float32)\n",
    "    carry_state = tf.zeros([1,128], dtype=np.float32)\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        action_probs, _, memory_state, carry_state = model(state, memory_state, carry_state)\n",
    "        \n",
    "        action_dist = tfd.Categorical(probs=action_probs)\n",
    "        action_index = int(action_dist.sample()[0])\n",
    "        #print(\"action_index: \", action_index)\n",
    "        #if random.random() <= 0.01:\n",
    "        #    action_index = random.randint(0,18)\n",
    "        #else:\n",
    "        #    action_index = np.argmax(np.squeeze(action_probs))\n",
    "        #print(\"action_index: \", action_index)\n",
    "        \n",
    "        action = env.action_space.noop()\n",
    "        if (action_index == 0):\n",
    "            action['camera'] = [0, -5]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 1):\n",
    "            action['camera'] = [0, -5]\n",
    "            action['attack'] = 1\n",
    "        elif (action_index == 2):\n",
    "            action['camera'] = [0, 5]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 3):\n",
    "            action['camera'] = [0, 5]\n",
    "            action['attack'] = 1\n",
    "        elif (action_index == 4):\n",
    "            action['camera'] = [-5, 0]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 5):\n",
    "            action['camera'] = [-5, 0]\n",
    "            action['attack'] = 1\n",
    "        elif (action_index == 6):\n",
    "            action['camera'] = [5, 0]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 7):\n",
    "            action['camera'] = [5, 0]\n",
    "            \n",
    "        elif (action_index == 8):\n",
    "            action['forward'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 9):\n",
    "            action['forward'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 10):\n",
    "            action['jump'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 11):\n",
    "            action['jump'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 12):\n",
    "            action['back'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 13):\n",
    "            action['back'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 14):\n",
    "            action['left'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 15):\n",
    "            action['left'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 16):\n",
    "            action['right'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 17):\n",
    "            action['right'] = 1\n",
    "            action['attack'] = 1 \n",
    "            \n",
    "        elif (action_index == 18):\n",
    "            action['sneak'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 19):\n",
    "            action['sneak'] = 1\n",
    "            action['attack'] = 1 \n",
    "        \n",
    "        observation_1, reward, done, info = env.step(action)\n",
    "        render(observation_1['pov'])\n",
    "        \n",
    "        inventory_channel_1 = np.zeros((64,64,1))\n",
    "        if 'inventory' in observation_1:\n",
    "            region_max_height = observation_1['pov'].shape[0]\n",
    "            region_max_width = observation_1['pov'].shape[1]\n",
    "            rs = 8\n",
    "            if min(region_max_height, region_max_width) < rs:\n",
    "                raise ValueError(\"'region_size' is too large.\")\n",
    "                \n",
    "            num_element_width = region_max_width // rs\n",
    "\n",
    "            inventory_channel_1 = np.zeros(shape=list(observation_1['pov'].shape[:-1]) + [1], \n",
    "                                           dtype=observation_1['pov'].dtype)\n",
    "            #print(\"state['inventory'].keys(): \" + str(state['inventory'].keys()))\n",
    "            for key_idx, key in enumerate(observation_1['inventory'].keys()):\n",
    "                #print(\"key.shape : \" + str(key))\n",
    "                #print(\"state['inventory'][key][i] : \" + str(state['inventory'][key][i]))\n",
    "                item_scaled = np.clip(1 - 1 / (observation_1['inventory'][key] + 1),  # Inversed\n",
    "                                      0, 1)\n",
    "                #print(\"item_scaled : \" + str(item_scaled))\n",
    "                item_channel = np.ones(shape=[rs, rs, 1], dtype=observation_1['pov'].dtype) * item_scaled\n",
    "                width_low = (key_idx % num_element_width) * rs\n",
    "                height_low = (key_idx // num_element_width) * rs\n",
    "\n",
    "                if height_low + rs > region_max_height:\n",
    "                    raise ValueError(\"Too many elements on 'inventory'. Please decrease 'region_size' of each component.\")\n",
    "\n",
    "                inventory_channel_1[height_low:(height_low + rs), width_low:(width_low + rs), :] = item_channel\n",
    "\n",
    "        next_state = np.concatenate((observation_1['pov'] / 255.0, inventory_channel_1), axis=2)\n",
    "        next_state = tf.constant(next_state, dtype=tf.float32)\n",
    "        \n",
    "        reward_sum += reward\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"Total reward: {:.2f},  Total step: {:.2f}\".format(reward_sum, step))\n",
    "            step = 0\n",
    "            reward_sum = 0  \n",
    "            #observation = env.reset()\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
