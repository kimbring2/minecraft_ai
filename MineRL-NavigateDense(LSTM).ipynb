{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-17 03:55:33.107925: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "/home/kimbring2/.local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2021-10-17 03:55:35.085009: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-17 03:55:35.085754: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-10-17 03:55:35.108859: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-10-17 03:55:35.108894: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: kimbring2-GF75-Thin-10UEK\n",
      "2021-10-17 03:55:35.108899: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: kimbring2-GF75-Thin-10UEK\n",
      "2021-10-17 03:55:35.108976: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.91.3\n",
      "2021-10-17 03:55:35.109000: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
      "2021-10-17 03:55:35.109008: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.91.3\n",
      "2021-10-17 03:55:35.109618: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import glob\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "import gym\n",
    "import minerl\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "#            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "workspace_path = '/home/kimbring2/minecraft_ai'\n",
    "\n",
    "writer = tf.summary.create_file_writer(workspace_path + \"/tensorboard\")\n",
    "\n",
    "env = gym.make('MineRLNavigateDense-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "  def __init__(\n",
    "      self, \n",
    "      num_actions: int, \n",
    "      num_hidden_units: int):\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_actions = num_actions\n",
    "    \n",
    "    self.conv_1 = layers.Conv2D(16, 8, 4, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.conv_2 = layers.Conv2D(32, 4, 2, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.conv_3 = layers.Conv2D(32, 3, 1, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    \n",
    "    self.lstm = layers.LSTM(128, return_sequences=True, return_state=True, kernel_regularizer='l2')\n",
    "    \n",
    "    self.common = layers.Dense(num_hidden_units, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.actor = layers.Dense(num_actions, kernel_regularizer='l2')\n",
    "    self.critic = layers.Dense(1, kernel_regularizer='l2')\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super().get_config().copy()\n",
    "    config.update({\n",
    "        'num_actions': self.num_actions,\n",
    "        'num_hidden_units': self.num_hidden_units\n",
    "    })\n",
    "    return config\n",
    "    \n",
    "  def call(self, inputs: tf.Tensor, memory_state: tf.Tensor, carry_state: tf.Tensor, training) -> Tuple[tf.Tensor, tf.Tensor, \n",
    "                                                                                                        tf.Tensor, tf.Tensor]:\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    conv_1 = self.conv_1(inputs)\n",
    "    conv_2 = self.conv_2(conv_1)\n",
    "    conv_3 = self.conv_3(conv_2)\n",
    "    #print(\"conv_3.shape: \", conv_3.shape)\n",
    "    conv_3_reshaped = layers.Reshape((4*4,32))(conv_3)\n",
    "    \n",
    "    initial_state = (memory_state, carry_state)\n",
    "    #print(\"initial_state: \", initial_state)\n",
    "    lstm_output, final_memory_state, final_carry_state  = self.lstm(conv_3_reshaped, initial_state=initial_state, \n",
    "                                                                    training=training)\n",
    "    \n",
    "    X_input = layers.Flatten()(lstm_output)\n",
    "    x = self.common(X_input)\n",
    "    \n",
    "    return tf.keras.layers.Softmax()(self.actor(x)), self.critic(x), final_memory_state, final_carry_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 20\n",
    "num_hidden_units = 512\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(reward, dones):\n",
    "    # Compute the gamma-discounted rewards over an episode\n",
    "    gamma = 0.99    # discount rate\n",
    "    running_add = 0\n",
    "    discounted_r = np.zeros_like(reward)\n",
    "    for i in reversed(range(0, len(reward))):\n",
    "        running_add = running_add * gamma * (1 - dones[i]) + reward[i]\n",
    "        discounted_r[i] = running_add\n",
    "\n",
    "    if np.std(discounted_r) != 0:\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= np.std(discounted_r) # divide by standard deviation\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "def take_vector_elements(vectors, indices):\n",
    "    return tf.gather_nd(vectors, tf.stack([tf.range(tf.shape(vectors)[0]), indices], axis=1))\n",
    "\n",
    "\n",
    "def render(obs):\n",
    "    obs = cv2.cvtColor(obs, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('obs', obs)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "\n",
    "@tf.function\n",
    "def reinforcement_replay(input_list, action_list, memory_state, carry_state, reward_list, done_list):\n",
    "    input_array = tf.concat(input_list, 0)\n",
    "    action_array = tf.concat(action_list, 0)\n",
    "    memory_state = tf.concat(memory_state, 0)\n",
    "    carry_state = tf.concat(carry_state, 0)\n",
    "\n",
    "    batch_size = input_array.shape[0]\n",
    "    \n",
    "    discounted_r = discount_rewards(reward_list, done_list)\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_logits = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        for i in tf.range(0, batch_size):\n",
    "            model_input = tf.expand_dims(input_array[i,:,:,:], 0)\n",
    "            prediction = model(model_input, memory_state, carry_state, training=False)\n",
    "            action_logit = prediction[0]\n",
    "            value = prediction[1]\n",
    "            memory_state = prediction[2]\n",
    "            carry_state = prediction[3]\n",
    "\n",
    "            action_logits = action_logits.write(i, action_logit[0])\n",
    "            values = values.write(i, tf.squeeze(value))\n",
    "            \n",
    "        action_logits = action_logits.stack()\n",
    "        values = values.stack()\n",
    "\n",
    "        action_logits_selected = take_vector_elements(action_logits, action_list)\n",
    "\n",
    "        print(\"discounted_r.shape: \", discounted_r.shape)\n",
    "        print(\"values.shape: \", values.shape)\n",
    "        discounted_r = tf.cast(discounted_r, 'float32')\n",
    "        advantages = discounted_r - values\n",
    "            \n",
    "        action_logits_selected_probs = tf.math.log(action_logits_selected)\n",
    "        \n",
    "        print(\"action_logits_selected_probs.shape: \", action_logits_selected_probs.shape)\n",
    "        print(\"advantages.shape: \", advantages.shape)\n",
    "        actor_loss = -tf.math.reduce_mean(action_logits_selected_probs * tf.stop_gradient(advantages)) \n",
    "        actor_loss = tf.cast(actor_loss, 'float32')\n",
    "            \n",
    "        critic_loss = mse_loss(values, discounted_r)\n",
    "        critic_loss = tf.cast(critic_loss, 'float32')\n",
    "        total_loss = actor_loss + 0.5 * critic_loss\n",
    "        \n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return total_loss, memory_state, carry_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discounted_r.shape:  (32,)\n",
      "values.shape:  (32,)\n",
      "action_logits_selected_probs.shape:  (32,)\n",
      "advantages.shape:  (32,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_82005/2718707359.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtraining_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mreinforcement_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_82005/2718707359.py\u001b[0m in \u001b[0;36mreinforcement_train\u001b[0;34m(training_episode)\u001b[0m\n\u001b[1;32m    111\u001b[0m                                                                          \u001b[0minitial_memory_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                                                                          \u001b[0minitial_carry_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                                                                          reward_list, done_list)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_82005/2256633278.py\u001b[0m in \u001b[0;36mreinforcement_replay\u001b[0;34m(input_list, action_list, memory_state, carry_state, reward_list, done_list)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1322\u001b[0m       \u001b[0m_ShapesFullySpecifiedAndEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       grad.dtype in (dtypes.int32, dtypes.float32)):\n\u001b[0;32m-> 1324\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" vs. \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6063\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6064\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 6065\u001b[0;31m         _ctx, \"Mul\", name, x, y)\n\u001b[0m\u001b[1;32m   6066\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6067\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def reinforcement_train(training_episode):\n",
    "    total_reward, done, SAVING = 0, False, ''\n",
    "    obs = env.reset()\n",
    "    \n",
    "    input_list, action_list, reward_list, done_list = [], [], [], []\n",
    "    \n",
    "    memory_state = tf.zeros([1,128], dtype=np.float32)\n",
    "    carry_state = tf.zeros([1,128], dtype=np.float32)\n",
    "    \n",
    "    initial_memory_state = memory_state\n",
    "    initial_carry_state = carry_state\n",
    "    while not done:\n",
    "        render(obs['pov'])\n",
    "        pov_array = obs['pov'] / 255.0\n",
    "        \n",
    "        compassAngle_array = obs['compass']['angle'] / 360.0\n",
    "        compassAngle_array = np.ones((64,64,1)) * compassAngle_array\n",
    "        #print(\"compassAngle_array.shape: \", compassAngle_array.shape)\n",
    "        \n",
    "        input_array = np.concatenate((pov_array, compassAngle_array), 2)\n",
    "        input_array = np.expand_dims(input_array, 0)\n",
    "        #print(\"input_array.shape: \", input_array.shape)\n",
    "        \n",
    "        prediction = model(input_array, memory_state, carry_state, training=False)\n",
    "        act_pi = prediction[0]\n",
    "        memory_state = prediction[2]\n",
    "        carry_state = prediction[3]\n",
    "        \n",
    "        action_dist = tfd.Categorical(probs=act_pi)\n",
    "        action_index = int(action_dist.sample()[0])\n",
    "        \n",
    "        action = env.action_space.noop()\n",
    "        if (action_index == 0):\n",
    "            action['camera'] = [0, -5]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 1):\n",
    "            action['camera'] = [0, -5]\n",
    "            action['attack'] = 1\n",
    "        elif (action_index == 2):\n",
    "            action['camera'] = [0, 5]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 3):\n",
    "            action['camera'] = [0, 5]\n",
    "            action['attack'] = 1\n",
    "        elif (action_index == 4):\n",
    "            action['camera'] = [-5, 0]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 5):\n",
    "            action['camera'] = [-5, 0]\n",
    "            action['attack'] = 1\n",
    "        elif (action_index == 6):\n",
    "            action['camera'] = [5, 0]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 7):\n",
    "            action['camera'] = [5, 0]\n",
    "            \n",
    "        elif (action_index == 8):\n",
    "            action['forward'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 9):\n",
    "            action['forward'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 10):\n",
    "            action['jump'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 11):\n",
    "            action['jump'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 12):\n",
    "            action['back'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 13):\n",
    "            action['back'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 14):\n",
    "            action['left'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 15):\n",
    "            action['left'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 16):\n",
    "            action['right'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 17):\n",
    "            action['right'] = 1\n",
    "            action['attack'] = 1 \n",
    "            \n",
    "        elif (action_index == 18):\n",
    "            action['sneak'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 19):\n",
    "            action['sneak'] = 1\n",
    "            action['attack'] = 1 \n",
    "        \n",
    "        obs_1, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        input_list.append(input_array)\n",
    "        action_list.append(action_index)\n",
    "        reward_list.append(reward)\n",
    "        done_list.append(done)\n",
    "        \n",
    "        obs = obs_1\n",
    "        if len(input_list) == 32:   \n",
    "            total_loss, memory_state, carry_state = reinforcement_replay(input_list, action_list,\n",
    "                                                                         initial_memory_state, \n",
    "                                                                         initial_carry_state,\n",
    "                                                                         reward_list, done_list)\n",
    "\n",
    "            input_list, action_list, reward_list, done_list = [], [], [], []\n",
    "            \n",
    "            initial_memory_state = memory_state\n",
    "            initial_carry_state = carry_state\n",
    "            \n",
    "            print(\"total_loss: \", total_loss)\n",
    "            print(\"\")\n",
    "        \n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar(\"total_reward\", total_reward, step=training_episode)\n",
    "            writer.flush()\n",
    "\n",
    "        #if training_episode % 100 == 0:\n",
    "        #    model.save_weights(workspace_path + '/model/supervised_model_' + str(training_episode))\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "for training_episode in range(0, 2000000):\n",
    "    reinforcement_train(training_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minerl\n",
    "import gym\n",
    "env = gym.make('MineRLNavigateDense-v0')\n",
    "\n",
    "\n",
    "obs  = env.reset()\n",
    "done = False\n",
    "net_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.noop()\n",
    "\n",
    "    action['camera'] = [0, 0.03*obs[\"compassAngle\"]]\n",
    "    action['back'] = 0\n",
    "    action['forward'] = 1\n",
    "    action['jump'] = 1\n",
    "    action['attack'] = 1\n",
    "\n",
    "    obs, reward, done, info = env.step(\n",
    "        action)\n",
    "    \n",
    "    render(obs['pov'])\n",
    "\n",
    "    net_reward += reward\n",
    "    print(\"Total reward: \", net_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import gym\n",
    "import minerl\n",
    "\n",
    "model.load_weights(workspace_path + \"/model/supervised_model_12000\")\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('MineRLTreechop-v0')\n",
    "\n",
    "seed = 980\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "reward_sum = 0\n",
    "for i_episode in range(0, 10000):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    inventory_channel = np.zeros((64,64,1))\n",
    "    if 'inventory' in observation:\n",
    "        region_max_height = observation['pov'].shape[0]\n",
    "        region_max_width = observation['pov'].shape[1]\n",
    "        rs = 8\n",
    "        if min(region_max_height, region_max_width) < rs:\n",
    "            raise ValueError(\"'region_size' is too large.\")\n",
    "            \n",
    "        num_element_width = region_max_width // rs\n",
    "\n",
    "        inventory_channel = np.zeros(shape=list(observation['pov'].shape[:-1]) + [1], \n",
    "                                     dtype=observation['pov'].dtype)\n",
    "        #print(\"state['inventory'].keys(): \" + str(state['inventory'].keys()))\n",
    "        for key_idx, key in enumerate(observation['inventory'].keys()):\n",
    "            #print(\"key.shape : \" + str(key))\n",
    "            #print(\"state['inventory'][key][i] : \" + str(state['inventory'][key][i]))\n",
    "            item_scaled = np.clip(1 - 1 / (observation['inventory'][key] + 1),  # Inversed\n",
    "                                  0, 1)\n",
    "            #print(\"item_scaled : \" + str(item_scaled))\n",
    "            item_channel = np.ones(shape=[rs, rs, 1], dtype=observation['pov'].dtype) * item_scaled\n",
    "            width_low = (key_idx % num_element_width) * rs\n",
    "            height_low = (key_idx // num_element_width) * rs\n",
    "\n",
    "            if height_low + rs > region_max_height:\n",
    "                raise ValueError(\"Too many elements on 'inventory'. Please decrease 'region_size' of each component.\")\n",
    "\n",
    "            inventory_channel[height_low:(height_low + rs), width_low:(width_low + rs), :] = item_channel\n",
    "\n",
    "    state = np.concatenate((observation['pov'] / 255.0, inventory_channel), axis=2)\n",
    "    state = tf.constant(state, dtype=tf.float32)\n",
    "    \n",
    "    memory_state = tf.zeros([1,128], dtype=np.float32)\n",
    "    carry_state = tf.zeros([1,128], dtype=np.float32)\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        action_probs, _, memory_state, carry_state = model(state, memory_state, carry_state)\n",
    "        \n",
    "        action_dist = tfd.Categorical(probs=action_probs)\n",
    "        action_index = int(action_dist.sample()[0])\n",
    "        #print(\"action_index: \", action_index)\n",
    "        #if random.random() <= 0.01:\n",
    "        #    action_index = random.randint(0,18)\n",
    "        #else:\n",
    "        #    action_index = np.argmax(np.squeeze(action_probs))\n",
    "        #print(\"action_index: \", action_index)\n",
    "        \n",
    "        action = env.action_space.noop()\n",
    "        if (action_index == 0):\n",
    "            action['camera'] = [0, -5]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 1):\n",
    "            action['camera'] = [0, -5]\n",
    "            action['attack'] = 1\n",
    "        elif (action_index == 2):\n",
    "            action['camera'] = [0, 5]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 3):\n",
    "            action['camera'] = [0, 5]\n",
    "            action['attack'] = 1\n",
    "        elif (action_index == 4):\n",
    "            action['camera'] = [-5, 0]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 5):\n",
    "            action['camera'] = [-5, 0]\n",
    "            action['attack'] = 1\n",
    "        elif (action_index == 6):\n",
    "            action['camera'] = [5, 0]\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 7):\n",
    "            action['camera'] = [5, 0]\n",
    "            \n",
    "        elif (action_index == 8):\n",
    "            action['forward'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 9):\n",
    "            action['forward'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 10):\n",
    "            action['jump'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 11):\n",
    "            action['jump'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 12):\n",
    "            action['back'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 13):\n",
    "            action['back'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 14):\n",
    "            action['left'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 15):\n",
    "            action['left'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        elif (action_index == 16):\n",
    "            action['right'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 17):\n",
    "            action['right'] = 1\n",
    "            action['attack'] = 1 \n",
    "            \n",
    "        elif (action_index == 18):\n",
    "            action['sneak'] = 1\n",
    "            action['attack'] = 0\n",
    "        elif (action_index == 19):\n",
    "            action['sneak'] = 1\n",
    "            action['attack'] = 1 \n",
    "        \n",
    "        observation_1, reward, done, info = env.step(action)\n",
    "        render(observation_1['pov'])\n",
    "        \n",
    "        inventory_channel_1 = np.zeros((64,64,1))\n",
    "        if 'inventory' in observation_1:\n",
    "            region_max_height = observation_1['pov'].shape[0]\n",
    "            region_max_width = observation_1['pov'].shape[1]\n",
    "            rs = 8\n",
    "            if min(region_max_height, region_max_width) < rs:\n",
    "                raise ValueError(\"'region_size' is too large.\")\n",
    "                \n",
    "            num_element_width = region_max_width // rs\n",
    "\n",
    "            inventory_channel_1 = np.zeros(shape=list(observation_1['pov'].shape[:-1]) + [1], \n",
    "                                           dtype=observation_1['pov'].dtype)\n",
    "            #print(\"state['inventory'].keys(): \" + str(state['inventory'].keys()))\n",
    "            for key_idx, key in enumerate(observation_1['inventory'].keys()):\n",
    "                #print(\"key.shape : \" + str(key))\n",
    "                #print(\"state['inventory'][key][i] : \" + str(state['inventory'][key][i]))\n",
    "                item_scaled = np.clip(1 - 1 / (observation_1['inventory'][key] + 1),  # Inversed\n",
    "                                      0, 1)\n",
    "                #print(\"item_scaled : \" + str(item_scaled))\n",
    "                item_channel = np.ones(shape=[rs, rs, 1], dtype=observation_1['pov'].dtype) * item_scaled\n",
    "                width_low = (key_idx % num_element_width) * rs\n",
    "                height_low = (key_idx // num_element_width) * rs\n",
    "\n",
    "                if height_low + rs > region_max_height:\n",
    "                    raise ValueError(\"Too many elements on 'inventory'. Please decrease 'region_size' of each component.\")\n",
    "\n",
    "                inventory_channel_1[height_low:(height_low + rs), width_low:(width_low + rs), :] = item_channel\n",
    "\n",
    "        next_state = np.concatenate((observation_1['pov'] / 255.0, inventory_channel_1), axis=2)\n",
    "        next_state = tf.constant(next_state, dtype=tf.float32)\n",
    "        \n",
    "        reward_sum += reward\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"Total reward: {:.2f},  Total step: {:.2f}\".format(reward_sum, step))\n",
    "            step = 0\n",
    "            reward_sum = 0  \n",
    "            #observation = env.reset()\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
