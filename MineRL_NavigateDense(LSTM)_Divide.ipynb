{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "py37",
      "language": "python",
      "name": "py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "MineRL-NavigateDense(LSTM)_Divide.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "AIpDVFxLKIrf",
        "outputId": "e20eb3fd-85c9-4030-d3ba-4ee6a1126427"
      },
      "source": [
        "import collections\n",
        "import gym\n",
        "import numpy as np\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import glob\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from typing import Any, List, Sequence, Tuple\n",
        "import gym\n",
        "import minerl\n",
        "import os\n",
        "import cv2\n",
        "import tqdm\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "tfd = tfp.distributions\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "#tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
        "#            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "\n",
        "workspace_path = '/home/kimbring2/minecraft_ai'\n",
        "\n",
        "writer = tf.summary.create_file_writer(workspace_path + \"/tensorboard\")\n",
        "\n",
        "env = gym.make('MineRLNavigateDense-v0')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-10-18 04:52:21.570215: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "/home/kimbring2/.local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "2021-10-18 04:52:22.968496: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-10-18 04:52:22.968999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-10-18 04:52:22.998811: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-10-18 04:52:22.998877: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: kimbring2-GF75-Thin-10UEK\n",
            "2021-10-18 04:52:22.998892: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: kimbring2-GF75-Thin-10UEK\n",
            "2021-10-18 04:52:22.999031: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.91.3\n",
            "2021-10-18 04:52:22.999084: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
            "2021-10-18 04:52:22.999099: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.91.3\n",
            "2021-10-18 04:52:23.001092: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PROoHwFCKIrm"
      },
      "source": [
        "class ActorCritic(tf.keras.Model):\n",
        "  \"\"\"Combined actor-critic network.\"\"\"\n",
        "  def __init__(\n",
        "      self, \n",
        "      num_actions: int, \n",
        "      num_hidden_units: int):\n",
        "    \"\"\"Initialize.\"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_actions = num_actions\n",
        "    \n",
        "    self.conv_1 = layers.Conv2D(16, 8, 4, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
        "    self.conv_2 = layers.Conv2D(32, 4, 2, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
        "    self.conv_3 = layers.Conv2D(32, 3, 1, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
        "    \n",
        "    self.lstm = layers.LSTM(128, return_sequences=True, return_state=True, kernel_regularizer='l2')\n",
        "    \n",
        "    self.common = layers.Dense(num_hidden_units, activation=\"relu\", kernel_regularizer='l2')\n",
        "    self.actor = layers.Dense(num_actions, kernel_regularizer='l2')\n",
        "    self.critic = layers.Dense(1, kernel_regularizer='l2')\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    config.update({\n",
        "        'num_actions': self.num_actions,\n",
        "        'num_hidden_units': self.num_hidden_units\n",
        "    })\n",
        "    return config\n",
        "    \n",
        "  def call(self, inputs: tf.Tensor, memory_state: tf.Tensor, carry_state: tf.Tensor, training) -> Tuple[tf.Tensor, tf.Tensor, \n",
        "                                                                                                        tf.Tensor, tf.Tensor]:\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "    conv_1 = self.conv_1(inputs)\n",
        "    conv_2 = self.conv_2(conv_1)\n",
        "    conv_3 = self.conv_3(conv_2)\n",
        "    conv_3_reshaped = layers.Reshape((4*4,32))(conv_3)\n",
        "    \n",
        "    initial_state = (memory_state, carry_state)\n",
        "    lstm_output, final_memory_state, final_carry_state  = self.lstm(conv_3_reshaped, initial_state=initial_state, \n",
        "                                                                    training=training)\n",
        "    #lstm_output = conv_3_reshaped\n",
        "    X_input = layers.Flatten()(lstm_output)\n",
        "    x = self.common(X_input)\n",
        "    \n",
        "    return self.actor(x), self.critic(x), memory_state, carry_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYq61InXKIrn"
      },
      "source": [
        "num_actions = 3\n",
        "num_hidden_units = 512\n",
        "\n",
        "model = ActorCritic(num_actions, num_hidden_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7LrsgKtKIro"
      },
      "source": [
        "def discount_rewards(reward, dones):\n",
        "    # Compute the gamma-discounted rewards over an episode\n",
        "    gamma = 0.99    # discount rate\n",
        "    running_add = 0\n",
        "    discounted_r = np.zeros_like(reward)\n",
        "    for i in reversed(range(0, len(reward))):\n",
        "        running_add = running_add * gamma * (1 - dones[i]) + reward[i]\n",
        "        discounted_r[i] = running_add\n",
        "\n",
        "    if np.std(discounted_r) != 0:\n",
        "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
        "        discounted_r /= np.std(discounted_r) # divide by standard deviation\n",
        "\n",
        "    return discounted_r\n",
        "\n",
        "\n",
        "def take_vector_elements(vectors, indices):\n",
        "    return tf.gather_nd(vectors, tf.stack([tf.range(tf.shape(vectors)[0]), indices], axis=1))\n",
        "\n",
        "\n",
        "def render(obs):\n",
        "    obs = cv2.cvtColor(obs, cv2.COLOR_RGB2BGR)\n",
        "    cv2.imshow('obs', obs)\n",
        "    cv2.waitKey(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P_ydJ6jKIrp"
      },
      "source": [
        "mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam(0.0001)\n",
        "\n",
        "@tf.function\n",
        "def get_loss(state_array, action_list, memory_state, carry_state, discounted_r):\n",
        "    batch_size = state_array.shape[0]\n",
        "    \n",
        "    policies = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "    for i in tf.range(0, batch_size):\n",
        "        model_input = tf.expand_dims(state_array[i,:,:,:], 0)\n",
        "        \n",
        "        prediction = model(model_input, memory_state, carry_state, training=True)\n",
        "        policy = prediction[0]\n",
        "        value = prediction[1]\n",
        "        memory_state = prediction[2]\n",
        "        carry_state = prediction[3]\n",
        "\n",
        "        policies = policies.write(i, policy[0])\n",
        "        values = values.write(i, tf.squeeze(value))\n",
        "            \n",
        "    policies = policies.stack()\n",
        "    values = values.stack()\n",
        "    \n",
        "    policies_softmax = tf.nn.softmax(policies)\n",
        "    \n",
        "    policies_selected = take_vector_elements(policies, action_list)\n",
        "\n",
        "    discounted_r = tf.cast(discounted_r, 'float32')\n",
        "    \n",
        "    #print(\"discounted_r: \", discounted_r)\n",
        "    #print(\"values: \", values)\n",
        "    advantages = discounted_r - values\n",
        "    \n",
        "    logits_selected = tf.nn.softmax(policies_selected)\n",
        "    logits_selected_logs = tf.math.log(logits_selected)\n",
        "    \n",
        "    #print(\"logits_selected_logs: \", logits_selected_logs)\n",
        "    #print(\"advantages: \", advantages)\n",
        "    actor_loss = -tf.math.reduce_mean(logits_selected_logs * tf.stop_gradient(advantages)) \n",
        "    actor_loss = tf.cast(actor_loss, 'float32')\n",
        "    \n",
        "    critic_loss = tf.reduce_mean(tf.square(values - discounted_r))\n",
        "    critic_loss = tf.cast(critic_loss, 'float32')\n",
        "    #tf.print(\"critic_loss: \", critic_loss)\n",
        "    \n",
        "    entropy_loss = -tf.math.reduce_mean(policies_softmax * tf.math.log(policies_softmax))\n",
        "    #tf.print(\"entropy_loss: \", entropy_loss)\n",
        "    \n",
        "    total_loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy_loss\n",
        "    #tf.print(\"total_loss: \", total_loss)\n",
        "    #tf.print(\"\")\n",
        "\n",
        "    return total_loss, memory_state, carry_state\n",
        "\n",
        "\n",
        "def reinforcement_replay(state_list, action_list, memory_state, carry_state, reward_list, done_list):\n",
        "    state_array = tf.concat(state_list, 0)\n",
        "    action_array = tf.concat(action_list, 0)\n",
        "    \n",
        "    memory_state = tf.concat(memory_state, 0)\n",
        "    carry_state = tf.concat(carry_state, 0)\n",
        "    \n",
        "    discounted_r_array = discount_rewards(reward_list, done_list)\n",
        "    discounted_r = tf.concat(discounted_r_array, 0)\n",
        "    \n",
        "    divide_size = 64\n",
        "    batch_size = state_array.shape[0]\n",
        "    epoch_size = batch_size // divide_size\n",
        "    remain_size = batch_size - epoch_size * divide_size\n",
        "    for e in range(0, epoch_size):\n",
        "        with tf.GradientTape() as tape:\n",
        "            total_loss, memory_state, carry_state = get_loss(state_array[divide_size*e:divide_size*(e+1),:,:,:], \n",
        "                                                             action_array[divide_size*e:divide_size*(e+1)], \n",
        "                                                             memory_state, carry_state,\n",
        "                                                             discounted_r[divide_size*e:divide_size*(e+1)])\n",
        "            #print(\"total_loss: \", total_loss)\n",
        "        \n",
        "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        \n",
        "    if remain_size != 0:\n",
        "        with tf.GradientTape() as tape:\n",
        "            total_loss, _, _ = get_loss(state_array[divide_size*epoch_size:divide_size*epoch_size+remain_size,:,:,:], \n",
        "                                        action_array[divide_size*epoch_size:divide_size*epoch_size+remain_size],\n",
        "                                        memory_state, carry_state,\n",
        "                                        discounted_r[divide_size*epoch_size:divide_size*epoch_size+remain_size])\n",
        "                \n",
        "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Bx9yd1RoKIrp",
        "outputId": "fe818b3f-6390-435b-f9d0-7885addc2b7f"
      },
      "source": [
        "def reinforcement_train(training_episode):\n",
        "    total_reward, done, SAVING = 0, False, ''\n",
        "    obs = env.reset()\n",
        "    \n",
        "    state_list, action_list, reward_list, done_list = [], [], [], []\n",
        "    \n",
        "    memory_state = tf.zeros([1,128], dtype=np.float32)\n",
        "    carry_state = tf.zeros([1,128], dtype=np.float32)\n",
        "    \n",
        "    initial_memory_state = memory_state\n",
        "    initial_carry_state = carry_state\n",
        "    while True:\n",
        "        render(obs['pov'])\n",
        "        \n",
        "        pov_array = obs['pov'] / 255.0\n",
        "        \n",
        "        compassAngle_array = obs['compass']['angle'] / 360.0\n",
        "        compassAngle_array = np.ones((64,64,1)) * compassAngle_array\n",
        "        #print(\"compassAngle_array: \", compassAngle_array)\n",
        "        \n",
        "        state_array = np.concatenate((pov_array, compassAngle_array), 2)\n",
        "        state_array = np.expand_dims(state_array, 0)\n",
        "        #print(\"input_array: \", input_array)\n",
        "        \n",
        "        prediction = model(state_array, memory_state, carry_state, training=False)\n",
        "        act_pi = prediction[0]\n",
        "        memory_state = prediction[2]\n",
        "        carry_state = prediction[3]\n",
        "        \n",
        "        action_index = tf.random.categorical(act_pi, 1)\n",
        "        action_index = int(action_index)\n",
        "        #print(\"action_index: \", action_index)\n",
        "        \n",
        "        action = env.action_space.noop()\n",
        "        if (action_index == 0):\n",
        "            action['camera'] = [0, -2]\n",
        "        elif (action_index == 1):\n",
        "            action['camera'] = [0, 2]\n",
        "        elif (action_index == 2):\n",
        "            action['forward'] = 1\n",
        "            action['jump'] = 1\n",
        "            action['attack'] = 1\n",
        "            \n",
        "        obs_1, reward, done, info = env.step(action)\n",
        "        \n",
        "        total_reward += reward\n",
        "        \n",
        "        state_list.append(state_array)\n",
        "        action_list.append(action_index)\n",
        "        reward_list.append(reward)\n",
        "        done_list.append(done)\n",
        "        \n",
        "        obs = obs_1\n",
        "        if done:\n",
        "            print(\"total_reward: \", total_reward)\n",
        "            \n",
        "            with writer.as_default():\n",
        "                tf.summary.scalar(\"total_reward\", total_reward, step=training_episode)\n",
        "                writer.flush()\n",
        "                \n",
        "            break\n",
        "        \n",
        "    reinforcement_replay(state_list, action_list, initial_memory_state, initial_carry_state, \n",
        "                         reward_list, done_list)\n",
        "\n",
        "    state_list, action_list, reward_list, done_list = [], [], [], []\n",
        "    clear_output(wait=True)\n",
        "\n",
        "            \n",
        "max_episodes = 200000\n",
        "with tqdm.trange(max_episodes) as t:\n",
        "  for i in t:\n",
        "    #print(\"i: \", i)\n",
        "    reinforcement_train(i)\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        model.save_weights(workspace_path + '/model/' + str(i))\n",
        "        clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "  0%|                                  | 3/200000 [05:21<5561:36:39, 100.11s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTFNlBODKIrr"
      },
      "source": [
        "import collections\n",
        "import gym\n",
        "import numpy as np\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import glob\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from typing import Any, List, Sequence, Tuple\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "import gym\n",
        "import minerl\n",
        "\n",
        "model.load_weights(workspace_path + \"/model/supervised_model_12000\")\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('MineRLNavigateDense-v0')\n",
        "\n",
        "seed = 980\n",
        "env.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "reward_sum = 0\n",
        "for i_episode in range(0, 10000):\n",
        "    observation = env.reset()\n",
        "    \n",
        "    pov_state = observation['pov'] / 255.0\n",
        "    compassAngle = observation['compass']['angle'] / 360.0\n",
        "    compassAngle_state = np.ones((64,64,1)) * compassAngle\n",
        "        \n",
        "    state = np.concatenate((pov_state, compassAngle_state), 2)\n",
        "    state = tf.constant(state, dtype=tf.float32)\n",
        "    \n",
        "    memory_state = tf.zeros([1,128], dtype=np.float32)\n",
        "    carry_state = tf.zeros([1,128], dtype=np.float32)\n",
        "    step = 0\n",
        "    while True:\n",
        "        step += 1\n",
        "\n",
        "        state = tf.expand_dims(state, 0)\n",
        "        action_probs, _, memory_state, carry_state = model(state, memory_state, carry_state)\n",
        "        \n",
        "        action_dist = tfd.Categorical(probs=action_probs)\n",
        "        action_index = int(action_dist.sample()[0])\n",
        "        \n",
        "        action = env.action_space.noop()\n",
        "        if (action_index == 0):\n",
        "            action['camera'] = [0, -5]\n",
        "        elif (action_index == 1):\n",
        "            action['camera'] = [0, 5]\n",
        "        elif (action_index == 2):\n",
        "            action['forward'] = 1\n",
        "        elif (action_index == 3):\n",
        "            action['jump'] = 1\n",
        "            \n",
        "        observation_1, reward, done, info = env.step(action)\n",
        "        render(observation_1['pov'])\n",
        "        \n",
        "        pov_next_state = observation_1['pov'] / 255.0\n",
        "        compassAngle = observation_1['compass']['angle'] / 360.0\n",
        "        compassAngle_next_state = np.ones((64,64,1)) * compassAngle\n",
        "        \n",
        "        next_state = np.concatenate((pov_next_state, compassAngle_next_state), 2)\n",
        "        next_state = tf.constant(next_state, dtype=tf.float32)\n",
        "        \n",
        "        reward_sum += reward\n",
        "\n",
        "        state = next_state\n",
        "        if done:\n",
        "            print(\"Total reward: {:.2f},  Total step: {:.2f}\".format(reward_sum, step))\n",
        "            step = 0\n",
        "            reward_sum = 0  \n",
        "            #observation = env.reset()\n",
        "            break\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkjYk19YKIrr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}