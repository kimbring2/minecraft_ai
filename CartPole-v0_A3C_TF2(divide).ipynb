{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VND_tcjcI_Ez",
    "outputId": "3e257d42-2173-43f6-a94e-eccec03b720d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-23 07:34:19.365694: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-10-23 07:34:19.365741: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: kimbring2-GF75-Thin-10UEK\n",
      "2021-10-23 07:34:19.365746: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: kimbring2-GF75-Thin-10UEK\n",
      "2021-10-23 07:34:19.365816: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.91.3\n",
      "2021-10-23 07:34:19.365833: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
      "2021-10-23 07:34:19.365838: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.91.3\n",
      "2021-10-23 07:34:19.366686: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/20000, thread: 0, score: 32.0, average: 32.00 SAVING\n",
      "episode: 1/20000, thread: 0, score: 13.0, average: 22.50 \n",
      "episode: 2/20000, thread: 0, score: 15.0, average: 20.00 \n",
      "episode: 3/20000, thread: 0, score: 53.0, average: 28.25 \n",
      "episode: 4/20000, thread: 0, score: 11.0, average: 24.80 \n",
      "episode: 5/20000, thread: 0, score: 20.0, average: 24.00 \n",
      "episode: 6/20000, thread: 0, score: 32.0, average: 25.14 \n",
      "episode: 7/20000, thread: 0, score: 22.0, average: 24.75 \n",
      "episode: 8/20000, thread: 0, score: 37.0, average: 26.11 \n",
      "episode: 9/20000, thread: 0, score: 21.0, average: 25.60 \n",
      "episode: 10/20000, thread: 0, score: 43.0, average: 27.18 \n",
      "episode: 11/20000, thread: 0, score: 17.0, average: 26.33 \n",
      "episode: 12/20000, thread: 0, score: 40.0, average: 27.38 \n",
      "episode: 13/20000, thread: 0, score: 17.0, average: 26.64 \n",
      "episode: 14/20000, thread: 0, score: 12.0, average: 25.67 \n",
      "episode: 15/20000, thread: 0, score: 30.0, average: 25.94 \n",
      "episode: 16/20000, thread: 0, score: 25.0, average: 25.88 \n",
      "episode: 17/20000, thread: 0, score: 29.0, average: 26.06 \n",
      "episode: 18/20000, thread: 0, score: 17.0, average: 25.58 \n",
      "episode: 19/20000, thread: 0, score: 13.0, average: 24.95 \n",
      "episode: 20/20000, thread: 0, score: 56.0, average: 26.43 \n",
      "episode: 21/20000, thread: 0, score: 37.0, average: 26.91 \n",
      "episode: 22/20000, thread: 0, score: 20.0, average: 26.61 \n",
      "episode: 23/20000, thread: 0, score: 45.0, average: 27.38 \n",
      "episode: 24/20000, thread: 0, score: 25.0, average: 27.28 \n",
      "episode: 25/20000, thread: 0, score: 20.0, average: 27.00 \n",
      "episode: 26/20000, thread: 0, score: 112.0, average: 30.15 \n",
      "episode: 27/20000, thread: 0, score: 22.0, average: 29.86 \n",
      "episode: 28/20000, thread: 0, score: 22.0, average: 29.59 \n",
      "episode: 29/20000, thread: 0, score: 34.0, average: 29.73 \n",
      "episode: 30/20000, thread: 0, score: 37.0, average: 29.97 \n",
      "episode: 31/20000, thread: 0, score: 50.0, average: 30.59 \n",
      "episode: 32/20000, thread: 0, score: 31.0, average: 30.61 \n",
      "episode: 33/20000, thread: 0, score: 39.0, average: 30.85 \n",
      "episode: 34/20000, thread: 0, score: 23.0, average: 30.63 \n",
      "episode: 35/20000, thread: 0, score: 26.0, average: 30.50 \n",
      "episode: 36/20000, thread: 0, score: 39.0, average: 30.73 \n",
      "episode: 37/20000, thread: 0, score: 63.0, average: 31.58 \n",
      "episode: 38/20000, thread: 0, score: 24.0, average: 31.38 \n",
      "episode: 39/20000, thread: 0, score: 66.0, average: 32.25 SAVING\n",
      "episode: 40/20000, thread: 0, score: 75.0, average: 33.29 SAVING\n",
      "episode: 41/20000, thread: 0, score: 27.0, average: 33.14 \n",
      "episode: 42/20000, thread: 0, score: 27.0, average: 33.00 \n",
      "episode: 43/20000, thread: 0, score: 51.0, average: 33.41 SAVING\n",
      "episode: 44/20000, thread: 0, score: 85.0, average: 34.56 SAVING\n",
      "episode: 45/20000, thread: 0, score: 87.0, average: 35.70 SAVING\n",
      "episode: 46/20000, thread: 0, score: 27.0, average: 35.51 \n",
      "episode: 47/20000, thread: 0, score: 19.0, average: 35.17 \n",
      "episode: 48/20000, thread: 0, score: 88.0, average: 36.24 SAVING\n",
      "episode: 49/20000, thread: 0, score: 33.0, average: 36.18 \n",
      "episode: 50/20000, thread: 0, score: 24.0, average: 36.02 \n",
      "episode: 51/20000, thread: 0, score: 67.0, average: 37.10 SAVING\n",
      "episode: 52/20000, thread: 0, score: 73.0, average: 38.26 SAVING\n",
      "episode: 53/20000, thread: 0, score: 87.0, average: 38.94 SAVING\n",
      "episode: 54/20000, thread: 0, score: 25.0, average: 39.22 SAVING\n",
      "episode: 55/20000, thread: 0, score: 126.0, average: 41.34 SAVING\n",
      "episode: 56/20000, thread: 0, score: 200.0, average: 44.70 SAVING\n",
      "episode: 57/20000, thread: 0, score: 105.0, average: 46.36 SAVING\n",
      "episode: 58/20000, thread: 0, score: 103.0, average: 47.68 SAVING\n",
      "episode: 59/20000, thread: 0, score: 55.0, average: 48.36 SAVING\n",
      "episode: 60/20000, thread: 0, score: 82.0, average: 49.14 SAVING\n",
      "episode: 61/20000, thread: 0, score: 195.0, average: 52.70 SAVING\n",
      "episode: 62/20000, thread: 0, score: 162.0, average: 55.14 SAVING\n",
      "episode: 63/20000, thread: 0, score: 134.0, average: 57.48 SAVING\n",
      "episode: 64/20000, thread: 0, score: 15.0, average: 57.54 SAVING\n",
      "episode: 65/20000, thread: 0, score: 24.0, average: 57.42 \n",
      "episode: 66/20000, thread: 0, score: 17.0, average: 57.26 \n",
      "episode: 67/20000, thread: 0, score: 22.0, average: 57.12 \n",
      "episode: 68/20000, thread: 0, score: 15.0, average: 57.08 \n",
      "episode: 69/20000, thread: 0, score: 164.0, average: 60.10 SAVING\n",
      "episode: 70/20000, thread: 0, score: 35.0, average: 59.68 \n",
      "episode: 71/20000, thread: 0, score: 63.0, average: 60.20 SAVING\n",
      "episode: 72/20000, thread: 0, score: 168.0, average: 63.16 SAVING\n",
      "episode: 73/20000, thread: 0, score: 110.0, average: 64.46 SAVING\n",
      "episode: 74/20000, thread: 0, score: 13.0, average: 64.22 \n",
      "episode: 75/20000, thread: 0, score: 92.0, average: 65.66 SAVING\n",
      "episode: 76/20000, thread: 0, score: 112.0, average: 65.66 SAVING\n",
      "episode: 77/20000, thread: 0, score: 47.0, average: 66.16 SAVING\n",
      "episode: 78/20000, thread: 0, score: 103.0, average: 67.78 SAVING\n",
      "episode: 79/20000, thread: 0, score: 19.0, average: 67.48 \n",
      "episode: 80/20000, thread: 0, score: 18.0, average: 67.10 \n",
      "episode: 81/20000, thread: 0, score: 19.0, average: 66.48 \n",
      "episode: 82/20000, thread: 0, score: 11.0, average: 66.08 \n",
      "episode: 83/20000, thread: 0, score: 66.0, average: 66.62 \n",
      "episode: 84/20000, thread: 0, score: 34.0, average: 66.84 \n",
      "episode: 85/20000, thread: 0, score: 119.0, average: 68.70 SAVING\n",
      "episode: 86/20000, thread: 0, score: 200.0, average: 71.92 SAVING\n",
      "episode: 87/20000, thread: 0, score: 73.0, average: 72.12 SAVING\n",
      "episode: 88/20000, thread: 0, score: 171.0, average: 75.06 SAVING\n",
      "episode: 89/20000, thread: 0, score: 100.0, average: 75.74 SAVING\n",
      "episode: 90/20000, thread: 0, score: 200.0, average: 78.24 SAVING\n",
      "episode: 91/20000, thread: 0, score: 153.0, average: 80.76 SAVING\n",
      "episode: 92/20000, thread: 0, score: 34.0, average: 80.90 SAVING\n",
      "episode: 93/20000, thread: 0, score: 72.0, average: 81.32 SAVING\n",
      "episode: 94/20000, thread: 0, score: 42.0, average: 80.46 \n",
      "episode: 95/20000, thread: 0, score: 11.0, average: 78.94 \n",
      "episode: 96/20000, thread: 0, score: 28.0, average: 78.96 \n",
      "episode: 97/20000, thread: 0, score: 53.0, average: 79.64 \n",
      "episode: 98/20000, thread: 0, score: 65.0, average: 79.18 \n",
      "episode: 99/20000, thread: 0, score: 46.0, average: 79.44 \n",
      "episode: 100/20000, thread: 0, score: 35.0, average: 79.66 \n",
      "episode: 101/20000, thread: 0, score: 69.0, average: 79.70 \n",
      "episode: 102/20000, thread: 0, score: 40.0, average: 79.04 \n",
      "episode: 103/20000, thread: 0, score: 39.0, average: 78.08 \n",
      "episode: 104/20000, thread: 0, score: 107.0, average: 79.72 \n",
      "episode: 105/20000, thread: 0, score: 109.0, average: 79.38 \n",
      "episode: 106/20000, thread: 0, score: 130.0, average: 77.98 \n",
      "episode: 107/20000, thread: 0, score: 113.0, average: 78.14 \n",
      "episode: 108/20000, thread: 0, score: 67.0, average: 77.42 \n",
      "episode: 109/20000, thread: 0, score: 49.0, average: 77.30 \n",
      "episode: 110/20000, thread: 0, score: 14.0, average: 75.94 \n",
      "episode: 111/20000, thread: 0, score: 79.0, average: 73.62 \n",
      "episode: 112/20000, thread: 0, score: 14.0, average: 70.66 \n",
      "episode: 113/20000, thread: 0, score: 13.0, average: 68.24 \n",
      "episode: 114/20000, thread: 0, score: 15.0, average: 68.24 \n",
      "episode: 115/20000, thread: 0, score: 29.0, average: 68.34 \n",
      "episode: 116/20000, thread: 0, score: 87.0, average: 69.74 \n",
      "episode: 117/20000, thread: 0, score: 14.0, average: 69.58 \n",
      "episode: 118/20000, thread: 0, score: 88.0, average: 71.04 \n",
      "episode: 119/20000, thread: 0, score: 187.0, average: 71.50 \n",
      "episode: 120/20000, thread: 0, score: 29.0, average: 71.38 \n",
      "episode: 121/20000, thread: 0, score: 53.0, average: 71.18 \n",
      "episode: 122/20000, thread: 0, score: 20.0, average: 68.22 \n",
      "episode: 123/20000, thread: 0, score: 115.0, average: 68.32 \n",
      "episode: 124/20000, thread: 0, score: 83.0, average: 69.72 \n",
      "episode: 125/20000, thread: 0, score: 97.0, average: 69.82 \n",
      "episode: 126/20000, thread: 0, score: 87.0, average: 69.32 \n",
      "episode: 127/20000, thread: 0, score: 50.0, average: 69.38 \n",
      "episode: 128/20000, thread: 0, score: 25.0, average: 67.82 \n",
      "episode: 129/20000, thread: 0, score: 45.0, average: 68.34 \n",
      "episode: 130/20000, thread: 0, score: 83.0, average: 69.64 \n",
      "episode: 131/20000, thread: 0, score: 37.0, average: 70.00 \n",
      "episode: 132/20000, thread: 0, score: 83.0, average: 71.44 \n",
      "episode: 133/20000, thread: 0, score: 85.0, average: 71.82 \n",
      "episode: 134/20000, thread: 0, score: 121.0, average: 73.56 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 135/20000, thread: 0, score: 167.0, average: 74.52 \n",
      "episode: 136/20000, thread: 0, score: 105.0, average: 72.62 \n",
      "episode: 137/20000, thread: 0, score: 192.0, average: 75.00 \n",
      "episode: 138/20000, thread: 0, score: 92.0, average: 73.42 \n",
      "episode: 139/20000, thread: 0, score: 185.0, average: 75.12 \n",
      "episode: 140/20000, thread: 0, score: 28.0, average: 71.68 \n",
      "episode: 141/20000, thread: 0, score: 83.0, average: 70.28 \n",
      "episode: 142/20000, thread: 0, score: 144.0, average: 72.48 \n",
      "episode: 143/20000, thread: 0, score: 45.0, average: 71.94 \n",
      "episode: 144/20000, thread: 0, score: 200.0, average: 75.10 \n",
      "episode: 145/20000, thread: 0, score: 91.0, average: 76.70 \n",
      "episode: 146/20000, thread: 0, score: 36.0, average: 76.86 \n",
      "episode: 147/20000, thread: 0, score: 200.0, average: 79.80 \n",
      "episode: 148/20000, thread: 0, score: 166.0, average: 81.82 SAVING\n",
      "episode: 149/20000, thread: 0, score: 35.0, average: 81.60 \n",
      "episode: 150/20000, thread: 0, score: 111.0, average: 83.12 SAVING\n",
      "episode: 151/20000, thread: 0, score: 114.0, average: 84.02 SAVING\n",
      "episode: 152/20000, thread: 0, score: 126.0, average: 85.74 SAVING\n",
      "episode: 153/20000, thread: 0, score: 87.0, average: 86.70 SAVING\n",
      "episode: 154/20000, thread: 0, score: 200.0, average: 88.56 SAVING\n",
      "episode: 155/20000, thread: 0, score: 133.0, average: 89.04 SAVING\n",
      "episode: 156/20000, thread: 0, score: 200.0, average: 90.44 SAVING\n",
      "episode: 157/20000, thread: 0, score: 37.0, average: 88.92 \n",
      "episode: 158/20000, thread: 0, score: 152.0, average: 90.62 SAVING\n",
      "episode: 159/20000, thread: 0, score: 164.0, average: 92.92 SAVING\n",
      "episode: 160/20000, thread: 0, score: 200.0, average: 96.64 SAVING\n",
      "episode: 161/20000, thread: 0, score: 28.0, average: 95.62 \n",
      "episode: 162/20000, thread: 0, score: 44.0, average: 96.22 \n",
      "episode: 163/20000, thread: 0, score: 200.0, average: 99.96 SAVING\n",
      "episode: 164/20000, thread: 0, score: 200.0, average: 103.66 SAVING\n",
      "episode: 165/20000, thread: 0, score: 122.0, average: 105.52 SAVING\n",
      "episode: 166/20000, thread: 0, score: 64.0, average: 105.06 \n",
      "episode: 167/20000, thread: 0, score: 39.0, average: 105.56 SAVING\n",
      "episode: 168/20000, thread: 0, score: 15.0, average: 104.10 \n",
      "episode: 169/20000, thread: 0, score: 200.0, average: 104.36 \n",
      "episode: 170/20000, thread: 0, score: 56.0, average: 104.90 \n",
      "episode: 171/20000, thread: 0, score: 13.0, average: 104.10 \n",
      "episode: 172/20000, thread: 0, score: 200.0, average: 107.70 SAVING\n",
      "episode: 173/20000, thread: 0, score: 122.0, average: 107.84 SAVING\n",
      "episode: 174/20000, thread: 0, score: 22.0, average: 106.62 \n",
      "episode: 175/20000, thread: 0, score: 74.0, average: 106.16 \n",
      "episode: 176/20000, thread: 0, score: 61.0, average: 105.64 \n",
      "episode: 177/20000, thread: 0, score: 79.0, average: 106.22 \n",
      "episode: 178/20000, thread: 0, score: 11.0, average: 105.94 \n",
      "episode: 179/20000, thread: 0, score: 58.0, average: 106.20 \n",
      "episode: 180/20000, thread: 0, score: 14.0, average: 104.82 \n",
      "episode: 181/20000, thread: 0, score: 164.0, average: 107.36 \n",
      "episode: 182/20000, thread: 0, score: 19.0, average: 106.08 \n",
      "episode: 183/20000, thread: 0, score: 15.0, average: 104.68 \n",
      "episode: 184/20000, thread: 0, score: 174.0, average: 105.74 \n",
      "episode: 185/20000, thread: 0, score: 200.0, average: 106.40 \n",
      "episode: 186/20000, thread: 0, score: 145.0, average: 107.20 \n",
      "episode: 187/20000, thread: 0, score: 34.0, average: 104.04 \n",
      "episode: 188/20000, thread: 0, score: 160.0, average: 105.40 \n",
      "episode: 189/20000, thread: 0, score: 178.0, average: 105.26 \n",
      "episode: 190/20000, thread: 0, score: 147.0, average: 107.64 \n",
      "episode: 191/20000, thread: 0, score: 115.0, average: 108.28 SAVING\n",
      "episode: 192/20000, thread: 0, score: 127.0, average: 107.94 \n",
      "episode: 193/20000, thread: 0, score: 137.0, average: 109.78 SAVING\n",
      "episode: 194/20000, thread: 0, score: 123.0, average: 108.24 \n",
      "episode: 195/20000, thread: 0, score: 88.0, average: 108.18 \n",
      "episode: 196/20000, thread: 0, score: 79.0, average: 109.04 \n",
      "episode: 197/20000, thread: 0, score: 126.0, average: 107.56 \n",
      "episode: 198/20000, thread: 0, score: 132.0, average: 106.88 \n",
      "episode: 199/20000, thread: 0, score: 154.0, average: 109.26 \n",
      "episode: 200/20000, thread: 0, score: 200.0, average: 111.04 SAVING\n",
      "episode: 201/20000, thread: 0, score: 200.0, average: 112.76 SAVING\n",
      "episode: 202/20000, thread: 0, score: 200.0, average: 114.24 SAVING\n",
      "episode: 203/20000, thread: 0, score: 200.0, average: 116.50 SAVING\n",
      "episode: 204/20000, thread: 0, score: 191.0, average: 116.32 \n",
      "episode: 205/20000, thread: 0, score: 99.0, average: 115.64 \n",
      "episode: 206/20000, thread: 0, score: 96.0, average: 113.56 \n",
      "episode: 207/20000, thread: 0, score: 134.0, average: 115.50 \n",
      "episode: 208/20000, thread: 0, score: 34.0, average: 113.14 \n",
      "episode: 209/20000, thread: 0, score: 29.0, average: 110.44 \n",
      "episode: 210/20000, thread: 0, score: 46.0, average: 107.36 \n",
      "episode: 211/20000, thread: 0, score: 30.0, average: 107.40 \n",
      "episode: 212/20000, thread: 0, score: 200.0, average: 110.52 \n",
      "episode: 213/20000, thread: 0, score: 200.0, average: 110.52 \n",
      "episode: 214/20000, thread: 0, score: 200.0, average: 110.52 \n",
      "episode: 215/20000, thread: 0, score: 200.0, average: 112.08 \n",
      "episode: 216/20000, thread: 0, score: 200.0, average: 114.80 \n",
      "episode: 217/20000, thread: 0, score: 200.0, average: 118.02 SAVING\n",
      "episode: 218/20000, thread: 0, score: 69.0, average: 119.10 SAVING\n",
      "episode: 219/20000, thread: 0, score: 80.0, average: 116.70 \n",
      "episode: 220/20000, thread: 0, score: 114.0, average: 117.86 \n",
      "episode: 221/20000, thread: 0, score: 24.0, average: 118.08 \n",
      "episode: 222/20000, thread: 0, score: 200.0, average: 118.08 \n"
     ]
    }
   ],
   "source": [
    "# Tutorial by www.pylessons.com\n",
    "# Tutorial written for - Tensorflow 2.3.1\n",
    "\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "import threading\n",
    "from threading import Thread, Lock\n",
    "import time\n",
    "import tensorflow_probability as tfp\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class OurModel(tf.keras.Model):\n",
    "    def __init__(self, input_shape, action_space):\n",
    "        super(OurModel, self).__init__()\n",
    "        \n",
    "        self.dense_0 = Dense(128, activation='relu')\n",
    "        self.dense_1 = Dense(action_space)\n",
    "        self.dense_2 = Dense(1)\n",
    "        \n",
    "    def call(self, X_input):\n",
    "        X_input = self.dense_0(X_input)\n",
    "        action_logit = self.dense_1(X_input)\n",
    "        value = self.dense_2(X_input)\n",
    "        \n",
    "        return action_logit, value\n",
    "\n",
    "\n",
    "def safe_log(x):\n",
    "  \"\"\"Computes a safe logarithm which returns 0 if x is zero.\"\"\"\n",
    "  return tf.where(\n",
    "      tf.math.equal(x, 0),\n",
    "      tf.zeros_like(x),\n",
    "      tf.math.log(tf.math.maximum(1e-12, x)))\n",
    "\n",
    "\n",
    "def take_vector_elements(vectors, indices):\n",
    "    \"\"\"\n",
    "    For a batch of vectors, take a single vector component\n",
    "    out of each vector.\n",
    "    Args:\n",
    "      vectors: a [batch x dims] Tensor.\n",
    "      indices: an int32 Tensor with `batch` entries.\n",
    "    Returns:\n",
    "      A Tensor with `batch` entries, one for each vector.\n",
    "    \"\"\"\n",
    "    return tf.gather_nd(vectors, tf.stack([tf.range(tf.shape(vectors)[0]), indices], axis=1))\n",
    "\n",
    "\n",
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "sparse_ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "\n",
    "class A3CAgent:\n",
    "    # Actor-Critic Main Optimization Algorithm\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES, self.episode, self.max_average = 20000, 0, -21.0 # specific for pong\n",
    "        self.lock = Lock()\n",
    "        self.lr = 0.000025\n",
    "\n",
    "        self.ROWS = 80\n",
    "        self.COLS = 80\n",
    "        self.REM_STEP = 4\n",
    "\n",
    "        # Instantiate plot memory\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Save_Path = 'Models'\n",
    "        #self.state_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "        self.state_size = 4\n",
    "        \n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.path = '{}_A3C_{}'.format(self.env_name, self.lr)\n",
    "        self.model_name = os.path.join(self.Save_Path, self.path)\n",
    "\n",
    "        # Create Actor-Critic network model\n",
    "        self.ActorCritic = OurModel(input_shape=self.state_size, action_space=self.action_size)\n",
    "        \n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "\n",
    "    def act(self, state):\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.ActorCritic(state, training=False)\n",
    "        \n",
    "        action_prob = prediction[0]\n",
    "        action_logit = tf.nn.softmax(action_prob)\n",
    "        \n",
    "        dist = tfd.Categorical(probs=action_logit)\n",
    "        \n",
    "        return dist.sample()\n",
    "\n",
    "    def discount_rewards(self, reward):\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0, len(reward))):\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= np.std(discounted_r) # divide by standard deviation\n",
    "\n",
    "        return discounted_r\n",
    "\n",
    "    def get_loss(self, states, actions, discounted_r):\n",
    "        discounted_r_vstack = np.vstack(discounted_r)\n",
    "        prediction = self.ActorCritic(states, training=True)\n",
    "        policies = prediction[0]\n",
    "        values = prediction[1]\n",
    "\n",
    "        policies_softmax = tf.nn.softmax(policies)\n",
    "\n",
    "        policies_selected = take_vector_elements(policies, actions)\n",
    "\n",
    "        advantages = discounted_r - np.stack(values)[:, 0] \n",
    "\n",
    "        logits_selected = tf.nn.softmax(policies_selected)\n",
    "        logits_selected_logs = tf.math.log(logits_selected)\n",
    "\n",
    "        #print(\"logits_selected_logs: \", logits_selected_logs)\n",
    "        #print(\"advantages.shape: \", advantages.shape)\n",
    "        actor_loss = -tf.math.reduce_mean(logits_selected_logs * advantages)\n",
    "        actor_loss = tf.cast(actor_loss, 'float32')\n",
    "\n",
    "        #critic_loss_ = huber_loss(values, discounted_r)\n",
    "        discounted_r = tf.cast(discounted_r, 'float32')\n",
    "\n",
    "        #print(\"values: \", values)\n",
    "        #print(\"discounted_r_vstack: \", discounted_r_vstack)\n",
    "        #critic_loss = huber_loss(values, discounted_r_vstack)\n",
    "        critic_loss = tf.reduce_mean(tf.square(values - discounted_r_vstack))\n",
    "\n",
    "        #entropy_loss = -tf.math.reduce_sum(policies_softmax * tf.math.log(policies_softmax))\n",
    "        #print(\"entropy_loss: \", entropy_loss)\n",
    "\n",
    "        total_loss = actor_loss + 0.5 * critic_loss\n",
    "            \n",
    "        return total_loss\n",
    "        \n",
    "    def replay(self, states, actions, rewards):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = tf.concat(states, 0)\n",
    "        \n",
    "        actions = tf.concat(actions, 0)\n",
    "        actions = tf.cast(actions, 'int32')\n",
    "        \n",
    "        # Compute discounted rewards\n",
    "        discounted_r = self.discount_rewards(rewards)\n",
    "        \n",
    "        divide_size = 2\n",
    "        batch_size = states.shape[0]\n",
    "        epoch_size = batch_size // divide_size\n",
    "        remain_size = batch_size - epoch_size * divide_size\n",
    "        #print(\"batch_size: \", batch_size)\n",
    "        #print(\"epoch_size: \", epoch_size)\n",
    "        #print(\"remain_size: \", remain_size)\n",
    "        \n",
    "        '''\n",
    "        batch_size:  23\n",
    "        epoch_size:  1\n",
    "        remain_size:  7\n",
    "        '''\n",
    "        for e in range(0, epoch_size):\n",
    "            #print(\"e: \", e)\n",
    "            with tf.GradientTape() as tape:\n",
    "                total_loss = self.get_loss(states[divide_size*e:divide_size*(e+1),:], \n",
    "                                           actions[divide_size*e:divide_size*(e+1)], \n",
    "                                           discounted_r[divide_size*e:divide_size*(e+1)])\n",
    "                \n",
    "            grads = tape.gradient(total_loss, self.ActorCritic.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.ActorCritic.trainable_variables))\n",
    "        \n",
    "        if remain_size != 0:\n",
    "            #print(\"remain_size: \", remain_size)\n",
    "            with tf.GradientTape() as tape:\n",
    "                total_loss = self.get_loss(states[divide_size*epoch_size:divide_size*epoch_size+remain_size,:], \n",
    "                                           actions[divide_size*epoch_size:divide_size*epoch_size+remain_size], \n",
    "                                           discounted_r[divide_size*epoch_size:divide_size*epoch_size+remain_size])\n",
    "                \n",
    "            grads = tape.gradient(total_loss, self.ActorCritic.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.ActorCritic.trainable_variables))\n",
    "            \n",
    "        #print(\"total_loss: \", total_loss)\n",
    "        #print(\"\")\n",
    "            \n",
    "    def load(self, model_name):\n",
    "        self.ActorCritic = load_model(model_name, compile=False)\n",
    "\n",
    "    def save(self):\n",
    "        self.ActorCritic.save(self.model_name)\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            pylab.plot(self.episodes, self.scores, 'b')\n",
    "            pylab.plot(self.episodes, self.average, 'r')\n",
    "            pylab.ylabel('Score', fontsize=18)\n",
    "            pylab.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                pylab.savefig(self.path+\".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(self.Model_name+str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def reset(self, env):\n",
    "        image_memory = np.zeros(self.state_size)\n",
    "        state = env.reset()\n",
    "            \n",
    "        return state\n",
    "    \n",
    "    def step(self, action, env, image_memory):\n",
    "        next_state, reward, done, info = env.step(int(action))\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def train(self, n_threads):\n",
    "        self.env.close()\n",
    "        \n",
    "        # Instantiate one environment per thread\n",
    "        envs = [gym.make(self.env_name) for i in range(n_threads)]\n",
    "\n",
    "        # Create threads\n",
    "        threads = [threading.Thread(\n",
    "                target=self.train_threading,\n",
    "                daemon=True,\n",
    "                args=(self,\n",
    "                    envs[i],\n",
    "                    i)) for i in range(n_threads)]\n",
    "\n",
    "        for t in threads:\n",
    "            time.sleep(2)\n",
    "            t.start()\n",
    "            \n",
    "        for t in threads:\n",
    "            time.sleep(10)\n",
    "            t.join()\n",
    "            \n",
    "    def train_threading(self, agent, env, thread):\n",
    "        while self.episode < self.EPISODES:\n",
    "            # Reset episode\n",
    "            score, done, SAVING = 0, False, ''\n",
    "            state = self.reset(env)\n",
    "            state = np.expand_dims(state, 0)\n",
    "\n",
    "            states, actions, rewards = [], [], []\n",
    "            while not done:\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, _ = self.step(action, env, state)\n",
    "                next_state = np.expand_dims(next_state, 0)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(int(action))\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                score += reward\n",
    "                state = next_state\n",
    "            \n",
    "            self.lock.acquire()\n",
    "            self.replay(states, actions, rewards)\n",
    "            self.lock.release()\n",
    "\n",
    "            states, actions, rewards = [], [], []\n",
    "            \n",
    "            # Update episode count\n",
    "            with self.lock:\n",
    "                average = self.PlotModel(score, self.episode)\n",
    "                # saving best models\n",
    "                if average >= self.max_average:\n",
    "                    self.max_average = average\n",
    "                    #self.save()\n",
    "                    SAVING = \"SAVING\"\n",
    "                else:\n",
    "                    SAVING = \"\"\n",
    "\n",
    "                print(\"episode: {}/{}, thread: {}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, thread, score, average, SAVING))\n",
    "                if(self.episode < self.EPISODES):\n",
    "                    self.episode += 1\n",
    "\n",
    "        env.close()            \n",
    "\n",
    "    def test(self, Actor_name, Critic_name):\n",
    "        self.load(Actor_name, Critic_name)\n",
    "        for e in range(100):\n",
    "            state = self.reset(self.env)\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.Actor.predict(state))\n",
    "                state, reward, done, _ = self.step(action, self.env, state)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'CartPole-v0'\n",
    "    #env_name = 'Pong-v0'\n",
    "    agent = A3CAgent(env_name)\n",
    "    \n",
    "    #agent.run() # use as A2C\n",
    "    agent.train(n_threads=1) # use as A3C\n",
    "    #agent.test('Models/Pong-v0_A3C_2.5e-05_Actor.h5', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pAzcLHMI_FI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Pong-v0_A3C_TF2(CartPole).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
