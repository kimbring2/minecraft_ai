{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  1\n",
      "selected_logit:  tf.Tensor(0.48846331895377915, shape=(), dtype=float64)\n",
      "selected_log_prob:  tf.Tensor(-0.7164908994613548, shape=(), dtype=float64)\n",
      "log_probs:  tf.Tensor(-0.7164909, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "def take_vector_elements(vectors, indices):\n",
    "    return tf.gather_nd(vectors, tf.stack([tf.range(tf.shape(vectors)[0]), indices], axis=1))\n",
    "\n",
    "policy_array = np.array([0.523077488, 0.476922572])\n",
    "\n",
    "dist = tfd.Categorical(logits=policy_array)\n",
    "action = int(dist.sample())\n",
    "\n",
    "print(\"action: \", action)\n",
    "\n",
    "logit_array = tf.nn.softmax(policy_array)\n",
    "\n",
    "selected_logit = logit_array[action]\n",
    "print(\"selected_logit: \", selected_logit)\n",
    "selected_log_prob = tf.math.log(selected_logit)\n",
    "print(\"selected_log_prob: \", selected_log_prob)\n",
    "\n",
    "log_probs = dist.log_prob(action)\n",
    "print(\"log_probs: \", log_probs)\n",
    "\n",
    "#target_action_probs = tf.nn.softmax(target_action_probs)\n",
    "#target_action_log_probs = tf.math.log(target_action_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[86, 38, 77, 69]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "memory = deque(maxlen=100)\n",
    "for i in range(0, 100):\n",
    "    memory.append(i)\n",
    "    \n",
    "batch_size = 4\n",
    "random.sample(memory, batch_size)\n",
    "#random.sample(memory, batch_size)\n",
    "#memory[6:6+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/5000000, score: 12.0, average: 12.00 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 1/5000000, score: 16.0, average: 14.00 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 2/5000000, score: 34.0, average: 20.67 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 3/5000000, score: 28.0, average: 22.50 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 4/5000000, score: 11.0, average: 20.20 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 5/5000000, score: 10.0, average: 18.50 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 6/5000000, score: 41.0, average: 21.71 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 7/5000000, score: 12.0, average: 20.50 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 8/5000000, score: 39.0, average: 22.56 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 9/5000000, score: 14.0, average: 21.70 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 10/5000000, score: 14.0, average: 21.00 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 11/5000000, score: 11.0, average: 20.17 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 12/5000000, score: 19.0, average: 20.08 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 13/5000000, score: 13.0, average: 19.57 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 14/5000000, score: 20.0, average: 19.60 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 15/5000000, score: 18.0, average: 19.50 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 16/5000000, score: 10.0, average: 18.94 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 17/5000000, score: 13.0, average: 18.61 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 18/5000000, score: 18.0, average: 18.58 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 19/5000000, score: 10.0, average: 18.15 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 20/5000000, score: 12.0, average: 17.86 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 21/5000000, score: 22.0, average: 18.05 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 22/5000000, score: 9.0, average: 17.65 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 23/5000000, score: 30.0, average: 18.17 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 24/5000000, score: 19.0, average: 18.20 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 25/5000000, score: 15.0, average: 18.08 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 26/5000000, score: 28.0, average: 18.44 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 27/5000000, score: 17.0, average: 18.39 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 28/5000000, score: 14.0, average: 18.24 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 29/5000000, score: 14.0, average: 18.10 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 30/5000000, score: 17.0, average: 18.06 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 31/5000000, score: 19.0, average: 18.09 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n",
      "episode: 32/5000000, score: 24.0, average: 18.27 \n",
      "learner_logits.shape:  TensorShape([15, 2])\n",
      "agent_logits.shape:  TensorShape([15, 2])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten, Reshape, LSTM\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_probability as tfp\n",
    "import cv2\n",
    "import vtrace\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "\n",
    "class OurModel(tf.keras.Model):\n",
    "    def __init__(self, input_shape, action_space):\n",
    "        super(OurModel, self).__init__()\n",
    "        \n",
    "        self.dense_0 = Dense(128, activation='relu', kernel_initializer='lecun_normal')\n",
    "        self.dense_1 = Dense(action_space, kernel_initializer='lecun_normal')\n",
    "        self.dense_2 = Dense(1, kernel_initializer='lecun_normal')\n",
    "        \n",
    "    def call(self, X_input):\n",
    "        X_input = self.dense_0(X_input)\n",
    "        action_prob = self.dense_1(X_input)\n",
    "        value = self.dense_2(X_input)\n",
    "        \n",
    "        return action_prob, value\n",
    "\n",
    "\n",
    "def safe_log(x):\n",
    "  return tf.where(\n",
    "      tf.math.equal(x, 0),\n",
    "      tf.zeros_like(x),\n",
    "      tf.math.log(tf.math.maximum(1e-12, x)))\n",
    "\n",
    "\n",
    "def take_vector_elements(vectors, indices):\n",
    "    return tf.gather_nd(vectors, tf.stack([tf.range(tf.shape(vectors)[0]), indices], axis=1))\n",
    "    \n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env_name):    \n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name       \n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES = 5000000\n",
    "        \n",
    "        # Instantiate memory\n",
    "        memory_size = 5000\n",
    "        self.memory = []\n",
    "\n",
    "        self.batch_size = 16\n",
    "\n",
    "        self.Save_Path = 'Models'\n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.env_name + \"_IMPALA.h5\")\n",
    "\n",
    "        self.COLS = 4\n",
    "        \n",
    "        self.state_size = (self.COLS)\n",
    "        self.image_memory = np.zeros(self.state_size)\n",
    "        \n",
    "        self.model = self.make_model('Model', self.state_size, self.action_size)\n",
    "        \n",
    "        #self.model.summary()\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "        \n",
    "    def remember(self, state, action, policy, reward, next_state, done):\n",
    "        experience = state, action, policy, reward, next_state, done\n",
    "        self.memory.append((experience))\n",
    "\n",
    "    def act(self, state):\n",
    "        prediction = self.model(state, training=False)\n",
    "        \n",
    "        dist = tfd.Categorical(logits=prediction[0])\n",
    "        action = int(dist.sample()[0])\n",
    "        policy = prediction[0]\n",
    "        \n",
    "        return action, policy\n",
    "    \n",
    "    def make_model(self, name, input_shape, action_space):\n",
    "        state = tf.keras.Input(shape=input_shape)\n",
    "        head = OurModel(input_shape, action_space)(state)\n",
    "        model = tf.keras.Model(inputs=state, outputs=head, name=name)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    #@tf.function\n",
    "    def update(self, states, actions, agent_policies, rewards, next_states, dones):\n",
    "        online_variables = self.model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(online_variables)\n",
    "            \n",
    "            learner_outputs = self.model(states, training=True)\n",
    "            \n",
    "            agent_logits = tf.nn.softmax(agent_policies[:-1])\n",
    "            actions = actions[:-1]\n",
    "            rewards = rewards[1:]\n",
    "            dones = dones[1:]\n",
    "        \n",
    "            learner_policies = learner_outputs[0]\n",
    "            learner_logits = tf.nn.softmax(learner_policies[:-1])\n",
    "            \n",
    "            learner_values = learner_outputs[1]\n",
    "            learner_values = tf.squeeze(learner_values)\n",
    "            \n",
    "            bootstrap_value = learner_values[-1]\n",
    "            learner_values = learner_values[:-1]\n",
    "            \n",
    "            discounting = 0.99\n",
    "            discounts = tf.cast(~dones, tf.float32) * discounting\n",
    "            \n",
    "            actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "            \n",
    "            tf.print(\"learner_logits.shape: \", learner_logits.shape)\n",
    "            tf.print(\"agent_logits.shape: \", agent_logits.shape)\n",
    "            \n",
    "            target_action_probs = take_vector_elements(learner_logits, actions)\n",
    "            target_action_log_probs = tf.math.log(target_action_probs)\n",
    "            \n",
    "            behaviour_action_probs = take_vector_elements(agent_logits, actions)\n",
    "            behaviour_action_log_probs = tf.math.log(behaviour_action_probs)\n",
    "            \n",
    "            lambda_ = 1.0\n",
    "            \n",
    "            log_rhos = target_action_log_probs - behaviour_action_log_probs\n",
    "            \n",
    "            log_rhos = tf.convert_to_tensor(log_rhos, dtype=tf.float32)\n",
    "            discounts = tf.convert_to_tensor(discounts, dtype=tf.float32)\n",
    "            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "            values = tf.convert_to_tensor(learner_values, dtype=tf.float32)\n",
    "            bootstrap_value = tf.convert_to_tensor(bootstrap_value, dtype=tf.float32)\n",
    "            \n",
    "            clip_rho_threshold = tf.convert_to_tensor(1.0, dtype=tf.float32)\n",
    "            clip_pg_rho_threshold = tf.convert_to_tensor(1.0, dtype=tf.float32)\n",
    "            \n",
    "            rhos = tf.math.exp(log_rhos)\n",
    "            \n",
    "            clipped_rhos = tf.minimum(clip_rho_threshold, rhos, name='clipped_rhos')\n",
    "            \n",
    "            cs = tf.minimum(1.0, rhos, name='cs')\n",
    "            cs *= tf.convert_to_tensor(lambda_, dtype=tf.float32)\n",
    "\n",
    "            values_t_plus_1 = tf.concat([values[1:], tf.expand_dims(bootstrap_value, 0)], axis=0)\n",
    "            deltas = clipped_rhos * (rewards + discounts * values_t_plus_1 - values)\n",
    "        \n",
    "            #deltas -= np.mean(deltas) # normalizing the result\n",
    "            #deltas /= np.std(deltas) # divide by standard deviation\n",
    "        \n",
    "            acc = tf.zeros_like(bootstrap_value)\n",
    "            vs_minus_v_xs = []\n",
    "            for i in range(int(discounts.shape[0]) - 1, -1, -1):\n",
    "                discount, c, delta = discounts[i], cs[i], deltas[i]\n",
    "                acc = delta + discount * c * acc\n",
    "                vs_minus_v_xs.append(acc)  \n",
    "            \n",
    "            vs_minus_v_xs = vs_minus_v_xs[::-1]\n",
    "            \n",
    "            vs = tf.add(vs_minus_v_xs, values, name='vs')\n",
    "            vs_t_plus_1 = tf.concat([vs[1:], tf.expand_dims(bootstrap_value, 0)], axis=0)\n",
    "            clipped_pg_rhos = tf.minimum(clip_pg_rho_threshold, rhos, name='clipped_pg_rhos')\n",
    "            \n",
    "            pg_advantages = (clipped_pg_rhos * (rewards + discounts * vs_t_plus_1 - values))\n",
    "            \n",
    "            vs = tf.stop_gradient(vs)\n",
    "            pg_advantages = tf.stop_gradient(pg_advantages)\n",
    "            \n",
    "            actor_loss = -tf.reduce_mean(target_action_log_probs * pg_advantages)\n",
    "            \n",
    "            baseline_cost = 0.5\n",
    "            v_error = values - vs\n",
    "            critic_loss = baseline_cost * 0.5 * tf.reduce_mean(tf.square(v_error))\n",
    "            \n",
    "            total_loss = actor_loss + critic_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "    \n",
    "    def replay(self):\n",
    "        memory_len = len(self.memory)\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            start_index = random.randint(0, memory_len - self.batch_size)\n",
    "            minibatch = self.memory[start_index:start_index+self.batch_size]\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.state_size), dtype=np.float32)\n",
    "        action = np.zeros(self.batch_size, dtype=np.int32)\n",
    "        policy = np.zeros((self.batch_size, self.action_size), dtype=np.float32)\n",
    "        reward = np.zeros(self.batch_size, dtype=np.float32)\n",
    "        next_state = np.zeros((self.batch_size, self.state_size), dtype=np.float32)\n",
    "        done = np.zeros(self.batch_size, dtype=np.bool)\n",
    "      \n",
    "        for i in range(len(minibatch)):\n",
    "            state[i], action[i], policy[i], reward[i], next_state[i], done[i] = minibatch[i]\n",
    "\n",
    "        self.update(state, action, policy, reward, next_state, done)\n",
    "        \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        pylab.plot(self.episodes, self.average, 'r')\n",
    "        pylab.plot(self.episodes, self.scores, 'b')\n",
    "        pylab.ylabel('Score', fontsize=18)\n",
    "        pylab.xlabel('Games', fontsize=18)\n",
    "        try:\n",
    "            pylab.savefig(self.env_name + \"_IMAPLA.png\")\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        # no need to worry about model, when doing a lot of experiments\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.env_name + \"_IMAPLA.h5\")\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(\"cartpole\" + str(rem_step), image[:,:,rem_step])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step(self,action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def run(self):\n",
    "        max_average = 190.0\n",
    "        total_step = 0\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            state = np.expand_dims(state, 0)\n",
    "            \n",
    "            done = False\n",
    "            score = 0\n",
    "            SAVING = ''\n",
    "            while not done:\n",
    "                #self.env.render()\n",
    "                action, policy = self.act(state)\n",
    "                \n",
    "                next_state, reward, done, _ = self.step(action)\n",
    "                next_state = np.expand_dims(next_state, 0)\n",
    "                \n",
    "                self.remember(state, action, policy, reward / 200.0, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "\n",
    "                if done:\n",
    "                    # every episode, plot the result\n",
    "                    average = self.PlotModel(score, e)\n",
    "\n",
    "                    # saving best models\n",
    "                    if average >= max_average:\n",
    "                        max_average = average\n",
    "                        self.save(self.Model_name)\n",
    "                        SAVING = \"SAVING\"\n",
    "                    else:\n",
    "                        SAVING = \"\"\n",
    "\n",
    "                    self.save(self.Model_name)\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(e, self.EPISODES, score, average, SAVING))\n",
    "                    \n",
    "                    break\n",
    "                \n",
    "                if total_step % 10 == 0:\n",
    "                    # train model\n",
    "                    self.replay()\n",
    "                    \n",
    "                total_step += 1\n",
    "                    \n",
    "        # close environemnt when finish training\n",
    "        self.env.close()\n",
    "\n",
    "    def test(self, Model_name):\n",
    "        self.load(Model_name)\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                state, reward, done, _ = self.step(action)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "                    \n",
    "        self.env.close()\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'CartPole-v0'\n",
    "    agent = DQNAgent(env_name)\n",
    "    agent.run()\n",
    "    #agent.test('Models/Pong-v0_DDQN_CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
