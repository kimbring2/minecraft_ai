{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 06:04:45.920920: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "/home/kimbring2/.local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2021-10-20 06:04:47.772647: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-20 06:04:47.773182: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-10-20 06:04:47.803351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 06:04:47.803587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Laptop GPU computeCapability: 8.6\n",
      "coreClock: 1.402GHz coreCount: 30 deviceMemorySize: 5.81GiB deviceMemoryBandwidth: 268.26GiB/s\n",
      "2021-10-20 06:04:47.803606: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-20 06:04:47.805254: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-20 06:04:47.805287: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-20 06:04:47.805801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-20 06:04:47.805938: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-20 06:04:47.807487: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-20 06:04:47.807891: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-10-20 06:04:47.807971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-10-20 06:04:47.808038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 06:04:47.808288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 06:04:47.808488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-20 06:04:47.810277: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-20 06:04:47.810418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 06:04:47.810628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Laptop GPU computeCapability: 8.6\n",
      "coreClock: 1.402GHz coreCount: 30 deviceMemorySize: 5.81GiB deviceMemoryBandwidth: 268.26GiB/s\n",
      "2021-10-20 06:04:47.810641: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-20 06:04:47.810652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-20 06:04:47.810662: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-20 06:04:47.810671: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-20 06:04:47.810680: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-20 06:04:47.810689: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-20 06:04:47.810698: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-10-20 06:04:47.810708: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-10-20 06:04:47.810744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 06:04:47.810966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 06:04:47.811228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-20 06:04:47.811248: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-20 06:04:48.143871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-10-20 06:04:48.143892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-10-20 06:04:48.143897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-10-20 06:04:48.144021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 06:04:48.144297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 06:04:48.144547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 06:04:48.144742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4000 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import glob\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "import gym\n",
    "import minerl\n",
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "from IPython.display import clear_output\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "workspace_path = '/home/kimbring2/minecraft_ai'\n",
    "\n",
    "writer = tf.summary.create_file_writer(workspace_path + \"/tensorboard\")\n",
    "\n",
    "env = gym.make('MineRLNavigateDense-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "  def __init__(\n",
    "      self, \n",
    "      num_actions: int, \n",
    "      num_hidden_units: int):\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_actions = num_actions\n",
    "    \n",
    "    self.conv_1 = layers.Conv2D(16, 8, 4, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.conv_2 = layers.Conv2D(32, 4, 2, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.conv_3 = layers.Conv2D(32, 3, 1, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    \n",
    "    self.lstm = layers.LSTM(128, return_sequences=True, return_state=True, kernel_regularizer='l2')\n",
    "    \n",
    "    self.common = layers.Dense(num_hidden_units, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.actor = layers.Dense(num_actions, kernel_regularizer='l2')\n",
    "    self.critic = layers.Dense(1, kernel_regularizer='l2')\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super().get_config().copy()\n",
    "    config.update({\n",
    "        'num_actions': self.num_actions,\n",
    "        'num_hidden_units': self.num_hidden_units\n",
    "    })\n",
    "    return config\n",
    "    \n",
    "  def call(self, inputs: tf.Tensor, memory_state: tf.Tensor, carry_state: tf.Tensor, training) -> Tuple[tf.Tensor, tf.Tensor, \n",
    "                                                                                                        tf.Tensor, tf.Tensor]:\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    conv_1 = self.conv_1(inputs)\n",
    "    conv_2 = self.conv_2(conv_1)\n",
    "    conv_3 = self.conv_3(conv_2)\n",
    "    conv_3_reshaped = layers.Reshape((4*4,32))(conv_3)\n",
    "    \n",
    "    initial_state = (memory_state, carry_state)\n",
    "    lstm_output, final_memory_state, final_carry_state  = self.lstm(conv_3_reshaped, initial_state=initial_state, \n",
    "                                                                    training=training)\n",
    "    #lstm_output = conv_3_reshaped\n",
    "    X_input = layers.Flatten()(lstm_output)\n",
    "    x = self.common(X_input)\n",
    "    \n",
    "    return self.actor(x), self.critic(x), memory_state, carry_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 3\n",
    "num_hidden_units = 512\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)\n",
    "\n",
    "memory = []\n",
    "memory_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_vector_elements(vectors, indices):\n",
    "    return tf.gather_nd(vectors, tf.stack([tf.range(tf.shape(vectors)[0]), indices], axis=1))\n",
    "\n",
    "\n",
    "def render(obs):\n",
    "    obs = cv2.cvtColor(obs, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('obs', obs)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "\n",
    "@tf.function\n",
    "def update(states, actions, agent_policies, rewards, dones, memory_states, carry_states):\n",
    "    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "    agent_policies = tf.convert_to_tensor(agent_policies, dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "    dones = tf.convert_to_tensor(dones, dtype=tf.bool)\n",
    "    memory_states = tf.convert_to_tensor(memory_states, dtype=tf.float32)\n",
    "    carry_states = tf.convert_to_tensor(carry_states, dtype=tf.float32)\n",
    "    \n",
    "    batch_size = states.shape[0]\n",
    "    \n",
    "    online_variables = model.trainable_variables\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(online_variables)\n",
    "        \n",
    "        learner_policies = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        learner_values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        \n",
    "        memory_state = tf.expand_dims(memory_states[0], 0)\n",
    "        carry_state = tf.expand_dims(carry_states[0], 0)\n",
    "        for i in tf.range(0, batch_size):\n",
    "            learner_output = model(tf.expand_dims(states[i,:,:,:], 0), memory_state, carry_state,\n",
    "                                                  training=True)\n",
    "            learner_policy = learner_output[0]\n",
    "            learner_policy = tf.squeeze(learner_policy)\n",
    "            learner_policies = learner_policies.write(i, learner_policy)\n",
    "            \n",
    "            learner_value = learner_output[1]\n",
    "            learner_value = tf.squeeze(learner_value)\n",
    "            learner_values = learner_values.write(i, learner_value)\n",
    "            \n",
    "            memory_state = learner_output[2]\n",
    "            carry_state = learner_output[3]\n",
    "        \n",
    "        learner_policies = learner_policies.stack()\n",
    "        learner_values = learner_values.stack()\n",
    "        \n",
    "        learner_logits = tf.nn.softmax(learner_policies[:-1])\n",
    "        agent_logits = tf.nn.softmax(agent_policies[:-1])\n",
    "         \n",
    "        actions = actions[:-1]\n",
    "        rewards = rewards[1:]\n",
    "        dones = dones[1:]\n",
    "            \n",
    "        bootstrap_value = learner_values[-1]\n",
    "        learner_values = learner_values[:-1]\n",
    "            \n",
    "        discounting = 0.99\n",
    "        discounts = tf.cast(~dones, tf.float32) * discounting\n",
    "            \n",
    "        target_action_probs = take_vector_elements(learner_logits, actions)\n",
    "        target_action_log_probs = tf.math.log(target_action_probs)\n",
    "\n",
    "        behaviour_action_probs = take_vector_elements(agent_logits, actions)\n",
    "        behaviour_action_log_probs = tf.math.log(behaviour_action_probs)\n",
    "\n",
    "        lambda_ = 1.0\n",
    "\n",
    "        log_rhos = target_action_log_probs - behaviour_action_log_probs\n",
    "\n",
    "        log_rhos = tf.convert_to_tensor(log_rhos, dtype=tf.float32)\n",
    "        discounts = tf.convert_to_tensor(discounts, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        values = tf.convert_to_tensor(learner_values, dtype=tf.float32)\n",
    "        bootstrap_value = tf.convert_to_tensor(bootstrap_value, dtype=tf.float32)\n",
    "\n",
    "        clip_rho_threshold = tf.convert_to_tensor(1.0, dtype=tf.float32)\n",
    "        clip_pg_rho_threshold = tf.convert_to_tensor(1.0, dtype=tf.float32)\n",
    "\n",
    "        rhos = tf.math.exp(log_rhos)\n",
    "\n",
    "        clipped_rhos = tf.minimum(clip_rho_threshold, rhos, name='clipped_rhos')\n",
    "\n",
    "        cs = tf.minimum(1.0, rhos, name='cs')\n",
    "        cs *= tf.convert_to_tensor(lambda_, dtype=tf.float32)\n",
    "\n",
    "        values_t_plus_1 = tf.concat([values[1:], tf.expand_dims(bootstrap_value, 0)], axis=0)\n",
    "        deltas = clipped_rhos * (rewards + discounts * values_t_plus_1 - values)\n",
    "\n",
    "        acc = tf.zeros_like(bootstrap_value)\n",
    "        vs_minus_v_xs = []\n",
    "        for i in range(int(discounts.shape[0]) - 1, -1, -1):\n",
    "            discount, c, delta = discounts[i], cs[i], deltas[i]\n",
    "            acc = delta + discount * c * acc\n",
    "            vs_minus_v_xs.append(acc)  \n",
    "\n",
    "        vs_minus_v_xs = vs_minus_v_xs[::-1]\n",
    "\n",
    "        vs = tf.add(vs_minus_v_xs, values, name='vs')\n",
    "        vs_t_plus_1 = tf.concat([vs[1:], tf.expand_dims(bootstrap_value, 0)], axis=0)\n",
    "        clipped_pg_rhos = tf.minimum(clip_pg_rho_threshold, rhos, name='clipped_pg_rhos')\n",
    "\n",
    "        pg_advantages = (clipped_pg_rhos * (rewards + discounts * vs_t_plus_1 - values))\n",
    "\n",
    "        vs = tf.stop_gradient(vs)\n",
    "        pg_advantages = tf.stop_gradient(pg_advantages)\n",
    "\n",
    "        actor_loss = -tf.reduce_mean(target_action_log_probs * pg_advantages)\n",
    "\n",
    "        baseline_cost = 0.5\n",
    "        v_error = values - vs\n",
    "        critic_loss = baseline_cost * 0.5 * tf.reduce_mean(tf.square(v_error))\n",
    "\n",
    "        total_loss = actor_loss + critic_loss\n",
    "\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "def reinforcement_replay():\n",
    "    global memory\n",
    "    batch_size = 32\n",
    "    \n",
    "    memory_len = len(memory)\n",
    "    if len(memory) > batch_size:\n",
    "        start_index = random.randint(0, memory_len - batch_size)\n",
    "        minibatch = memory[start_index:start_index+batch_size]\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    states = np.zeros((batch_size, 64, 64, 4), dtype=np.float32)\n",
    "    actions = np.zeros(batch_size, dtype=np.int32)\n",
    "    policies = np.zeros((batch_size, num_actions), dtype=np.float32)\n",
    "    rewards = np.zeros(batch_size, dtype=np.float32)\n",
    "    dones = np.zeros(batch_size, dtype=np.bool)\n",
    "    memory_states = np.zeros((batch_size, 128), dtype=np.float32)\n",
    "    carry_states = np.zeros((batch_size, 128), dtype=np.float32)\n",
    "      \n",
    "    for i in range(len(minibatch)):\n",
    "        states[i] = minibatch[i][0]\n",
    "        actions[i] = minibatch[i][1]\n",
    "        policies[i] = minibatch[i][2]\n",
    "        rewards[i] = minibatch[i][3]\n",
    "        dones[i] = minibatch[i][4]\n",
    "        memory_states[i] = minibatch[i][5]\n",
    "        carry_states[i] = minibatch[i][6]\n",
    "\n",
    "    update(states, actions, policies, rewards, dones, memory_states, carry_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                  | 2/200000 [03:49<6296:50:44, 113.34s/it]"
     ]
    }
   ],
   "source": [
    "def reinforcement_train(training_episode):\n",
    "    global memory\n",
    "    total_reward, done, SAVING = 0, False, ''\n",
    "    obs = env.reset()\n",
    "    \n",
    "    memory_state = tf.zeros([1,128], dtype=tf.dtypes.float32)\n",
    "    carry_state = tf.zeros([1,128], dtype=tf.dtypes.float32)\n",
    "    \n",
    "    total_step = 0\n",
    "    while True:\n",
    "        render(obs['pov'])\n",
    "        \n",
    "        pov_array = obs['pov'] / 255.0\n",
    "        \n",
    "        compassAngle_array = obs['compass']['angle'] / 360.0\n",
    "        compassAngle_array = np.ones((64,64,1)) * compassAngle_array\n",
    "        \n",
    "        state_array = np.concatenate((pov_array, compassAngle_array), 2)\n",
    "        state_array = np.expand_dims(state_array, 0)\n",
    "        \n",
    "        prediction = model(state_array, memory_state, carry_state, training=False)\n",
    "        act_pi = prediction[0]\n",
    "        next_memory_state = prediction[2]\n",
    "        next_carry_state = prediction[3]\n",
    "        \n",
    "        action_index = tf.random.categorical(act_pi, 1)\n",
    "        action_index = int(action_index)\n",
    "        \n",
    "        action = env.action_space.noop()\n",
    "        if (action_index == 0):\n",
    "            action['camera'] = [0, -2]\n",
    "        elif (action_index == 1):\n",
    "            action['camera'] = [0, 2]\n",
    "        elif (action_index == 2):\n",
    "            action['forward'] = 1\n",
    "            action['jump'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        obs_1, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        experience = state_array, action_index, act_pi, reward / 100.0, done, memory_state, carry_state\n",
    "        \n",
    "        #print(\"len(memory): \", len(memory))\n",
    "        if len(memory) <= memory_size:\n",
    "            memory.append((experience))\n",
    "        else:\n",
    "            memory = []\n",
    "            \n",
    "        obs = obs_1\n",
    "        memory_state = next_memory_state\n",
    "        carry_state = next_carry_state\n",
    "        if done:\n",
    "            print(\"total_reward: \", total_reward)\n",
    "            \n",
    "            with writer.as_default():\n",
    "                tf.summary.scalar(\"total_reward\", total_reward, step=training_episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            break\n",
    "        \n",
    "        if total_step % 100 == 0:\n",
    "            # train model\n",
    "            reinforcement_replay()\n",
    "\n",
    "        total_step += 1\n",
    "            \n",
    "    clear_output(wait=True)\n",
    "\n",
    "            \n",
    "max_episodes = 200000\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "  for i in t:\n",
    "    #print(\"i: \", i)\n",
    "    reinforcement_train(i)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        model.save_weights(workspace_path + '/model/' + str(i))\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import gym\n",
    "import minerl\n",
    "\n",
    "model.load_weights(workspace_path + \"/model/supervised_model_12000\")\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('MineRLNavigateDense-v0')\n",
    "\n",
    "seed = 980\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "reward_sum = 0\n",
    "for i_episode in range(0, 10000):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    pov_state = observation['pov'] / 255.0\n",
    "    compassAngle = observation['compass']['angle'] / 360.0\n",
    "    compassAngle_state = np.ones((64,64,1)) * compassAngle\n",
    "        \n",
    "    state = np.concatenate((pov_state, compassAngle_state), 2)\n",
    "    state = tf.constant(state, dtype=tf.float32)\n",
    "    \n",
    "    memory_state = tf.zeros([1,128], dtype=np.float32)\n",
    "    carry_state = tf.zeros([1,128], dtype=np.float32)\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        action_probs, _, memory_state, carry_state = model(state, memory_state, carry_state)\n",
    "        \n",
    "        action_dist = tfd.Categorical(probs=action_probs)\n",
    "        action_index = int(action_dist.sample()[0])\n",
    "        \n",
    "        action = env.action_space.noop()\n",
    "        if (action_index == 0):\n",
    "            action['camera'] = [0, -5]\n",
    "        elif (action_index == 1):\n",
    "            action['camera'] = [0, 5]\n",
    "        elif (action_index == 2):\n",
    "            action['forward'] = 1\n",
    "        elif (action_index == 3):\n",
    "            action['jump'] = 1\n",
    "            \n",
    "        observation_1, reward, done, info = env.step(action)\n",
    "        render(observation_1['pov'])\n",
    "        \n",
    "        pov_next_state = observation_1['pov'] / 255.0\n",
    "        compassAngle = observation_1['compass']['angle'] / 360.0\n",
    "        compassAngle_next_state = np.ones((64,64,1)) * compassAngle\n",
    "        \n",
    "        next_state = np.concatenate((pov_next_state, compassAngle_next_state), 2)\n",
    "        next_state = tf.constant(next_state, dtype=tf.float32)\n",
    "        \n",
    "        reward_sum += reward\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"Total reward: {:.2f},  Total step: {:.2f}\".format(reward_sum, step))\n",
    "            step = 0\n",
    "            reward_sum = 0  \n",
    "            #observation = env.reset()\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
