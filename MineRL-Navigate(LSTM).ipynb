{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 00:57:14.179986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "/home/kimbring2/.local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2021-10-20 00:57:15.692808: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-20 00:57:15.693337: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-10-20 00:57:15.720979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 00:57:15.721205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Laptop GPU computeCapability: 8.6\n",
      "coreClock: 1.402GHz coreCount: 30 deviceMemorySize: 5.81GiB deviceMemoryBandwidth: 268.26GiB/s\n",
      "2021-10-20 00:57:15.721223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-20 00:57:15.722815: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-20 00:57:15.722845: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-20 00:57:15.723389: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-20 00:57:15.723517: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-20 00:57:15.724987: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-20 00:57:15.725383: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-10-20 00:57:15.725455: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-10-20 00:57:15.725511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 00:57:15.725748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 00:57:15.725937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-20 00:57:15.727572: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-20 00:57:15.727642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 00:57:15.727847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Laptop GPU computeCapability: 8.6\n",
      "coreClock: 1.402GHz coreCount: 30 deviceMemorySize: 5.81GiB deviceMemoryBandwidth: 268.26GiB/s\n",
      "2021-10-20 00:57:15.727862: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-20 00:57:15.727874: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-20 00:57:15.727883: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-20 00:57:15.727892: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-20 00:57:15.727902: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-20 00:57:15.727911: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-20 00:57:15.727920: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-10-20 00:57:15.727930: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-10-20 00:57:15.727965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 00:57:15.728180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 00:57:15.728364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-20 00:57:15.728384: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-20 00:57:16.073819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-10-20 00:57:16.073841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-10-20 00:57:16.073845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-10-20 00:57:16.073952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 00:57:16.074232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 00:57:16.074562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-20 00:57:16.074777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4000 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import glob\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "import gym\n",
    "import minerl\n",
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "from IPython.display import clear_output\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "workspace_path = '/home/kimbring2/minecraft_ai'\n",
    "\n",
    "writer = tf.summary.create_file_writer(workspace_path + \"/tensorboard\")\n",
    "\n",
    "env = gym.make('MineRLNavigateDense-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "  def __init__(\n",
    "      self, \n",
    "      num_actions: int, \n",
    "      num_hidden_units: int):\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_actions = num_actions\n",
    "    \n",
    "    self.conv_1 = layers.Conv2D(16, 8, 4, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.conv_2 = layers.Conv2D(32, 4, 2, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.conv_3 = layers.Conv2D(32, 3, 1, padding=\"valid\", activation=\"relu\", kernel_regularizer='l2')\n",
    "    \n",
    "    self.lstm = layers.LSTM(128, return_sequences=True, return_state=True, kernel_regularizer='l2')\n",
    "    \n",
    "    self.common = layers.Dense(num_hidden_units, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.actor = layers.Dense(num_actions, kernel_regularizer='l2')\n",
    "    self.critic = layers.Dense(1, kernel_regularizer='l2')\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super().get_config().copy()\n",
    "    config.update({\n",
    "        'num_actions': self.num_actions,\n",
    "        'num_hidden_units': self.num_hidden_units\n",
    "    })\n",
    "    return config\n",
    "    \n",
    "  def call(self, inputs: tf.Tensor, memory_state: tf.Tensor, carry_state: tf.Tensor, training) -> Tuple[tf.Tensor, tf.Tensor, \n",
    "                                                                                                        tf.Tensor, tf.Tensor]:\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    conv_1 = self.conv_1(inputs)\n",
    "    conv_2 = self.conv_2(conv_1)\n",
    "    conv_3 = self.conv_3(conv_2)\n",
    "    conv_3_reshaped = layers.Reshape((4*4,32))(conv_3)\n",
    "    \n",
    "    initial_state = (memory_state, carry_state)\n",
    "    lstm_output, final_memory_state, final_carry_state  = self.lstm(conv_3_reshaped, initial_state=initial_state, \n",
    "                                                                    training=training)\n",
    "    #lstm_output = conv_3_reshaped\n",
    "    X_input = layers.Flatten()(lstm_output)\n",
    "    x = self.common(X_input)\n",
    "    \n",
    "    return self.actor(x), self.critic(x), memory_state, carry_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 3\n",
    "num_hidden_units = 512\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)\n",
    "\n",
    "memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(reward, dones):\n",
    "    # Compute the gamma-discounted rewards over an episode\n",
    "    gamma = 0.99    # discount rate\n",
    "    running_add = 0\n",
    "    discounted_r = np.zeros_like(reward)\n",
    "    for i in reversed(range(0, len(reward))):\n",
    "        running_add = running_add * gamma * (1 - dones[i]) + reward[i]\n",
    "        discounted_r[i] = running_add\n",
    "\n",
    "    if np.std(discounted_r) != 0:\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= np.std(discounted_r) # divide by standard deviation\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "def take_vector_elements(vectors, indices):\n",
    "    return tf.gather_nd(vectors, tf.stack([tf.range(tf.shape(vectors)[0]), indices], axis=1))\n",
    "\n",
    "\n",
    "def render(obs):\n",
    "    obs = cv2.cvtColor(obs, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('obs', obs)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "\n",
    "@tf.function\n",
    "def update(states, actions, agent_policies, rewards, dones, memory_states, carry_states):\n",
    "    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "    agent_policies = tf.convert_to_tensor(agent_policies, dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "    dones = tf.convert_to_tensor(dones, dtype=tf.bool)\n",
    "    memory_states = tf.convert_to_tensor(memory_states, dtype=tf.float32)\n",
    "    carry_states = tf.convert_to_tensor(carry_states, dtype=tf.float32)\n",
    "    \n",
    "    batch_size = states.shape[0]\n",
    "    \n",
    "    online_variables = model.trainable_variables\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(online_variables)\n",
    "        \n",
    "        learner_policies = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        learner_values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        \n",
    "        memory_state = tf.expand_dims(memory_states[0], 0)\n",
    "        carry_state = tf.expand_dims(carry_states[0], 0)\n",
    "        for i in tf.range(0, batch_size):\n",
    "            learner_output = model(tf.expand_dims(states[i,:,:,:], 0), memory_state, carry_state,\n",
    "                                                  training=True)\n",
    "            learner_policy = learner_output[0]\n",
    "            learner_policy = tf.squeeze(learner_policy)\n",
    "            learner_policies = learner_policies.write(i, learner_policy)\n",
    "            \n",
    "            learner_value = learner_output[1]\n",
    "            learner_value = tf.squeeze(learner_value)\n",
    "            learner_values = learner_values.write(i, learner_value)\n",
    "            \n",
    "            memory_state = learner_output[2]\n",
    "            carry_state = learner_output[3]\n",
    "        \n",
    "        learner_policies = learner_policies.stack()\n",
    "        learner_values = learner_values.stack()\n",
    "        \n",
    "        learner_logits = tf.nn.softmax(learner_policies[:-1])\n",
    "        agent_logits = tf.nn.softmax(agent_policies[:-1])\n",
    "         \n",
    "        actions = actions[:-1]\n",
    "        rewards = rewards[1:]\n",
    "        dones = dones[1:]\n",
    "        \n",
    "        learner_logits = tf.nn.softmax(learner_policies[:-1])\n",
    "            \n",
    "        bootstrap_value = learner_values[-1]\n",
    "        learner_values = learner_values[:-1]\n",
    "            \n",
    "        discounting = 0.99\n",
    "        discounts = tf.cast(~dones, tf.float32) * discounting\n",
    "            \n",
    "        target_action_probs = take_vector_elements(learner_logits, actions)\n",
    "        target_action_log_probs = tf.math.log(target_action_probs)\n",
    "\n",
    "        behaviour_action_probs = take_vector_elements(agent_logits, actions)\n",
    "        behaviour_action_log_probs = tf.math.log(behaviour_action_probs)\n",
    "\n",
    "        lambda_ = 1.0\n",
    "\n",
    "        log_rhos = target_action_log_probs - behaviour_action_log_probs\n",
    "\n",
    "        log_rhos = tf.convert_to_tensor(log_rhos, dtype=tf.float32)\n",
    "        discounts = tf.convert_to_tensor(discounts, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        values = tf.convert_to_tensor(learner_values, dtype=tf.float32)\n",
    "        bootstrap_value = tf.convert_to_tensor(bootstrap_value, dtype=tf.float32)\n",
    "\n",
    "        clip_rho_threshold = tf.convert_to_tensor(1.0, dtype=tf.float32)\n",
    "        clip_pg_rho_threshold = tf.convert_to_tensor(1.0, dtype=tf.float32)\n",
    "\n",
    "        rhos = tf.math.exp(log_rhos)\n",
    "\n",
    "        clipped_rhos = tf.minimum(clip_rho_threshold, rhos, name='clipped_rhos')\n",
    "\n",
    "        cs = tf.minimum(1.0, rhos, name='cs')\n",
    "        cs *= tf.convert_to_tensor(lambda_, dtype=tf.float32)\n",
    "\n",
    "        values_t_plus_1 = tf.concat([values[1:], tf.expand_dims(bootstrap_value, 0)], axis=0)\n",
    "        deltas = clipped_rhos * (rewards + discounts * values_t_plus_1 - values)\n",
    "\n",
    "        acc = tf.zeros_like(bootstrap_value)\n",
    "        vs_minus_v_xs = []\n",
    "        for i in range(int(discounts.shape[0]) - 1, -1, -1):\n",
    "            discount, c, delta = discounts[i], cs[i], deltas[i]\n",
    "            acc = delta + discount * c * acc\n",
    "            vs_minus_v_xs.append(acc)  \n",
    "\n",
    "        vs_minus_v_xs = vs_minus_v_xs[::-1]\n",
    "\n",
    "        vs = tf.add(vs_minus_v_xs, values, name='vs')\n",
    "        vs_t_plus_1 = tf.concat([vs[1:], tf.expand_dims(bootstrap_value, 0)], axis=0)\n",
    "        clipped_pg_rhos = tf.minimum(clip_pg_rho_threshold, rhos, name='clipped_pg_rhos')\n",
    "\n",
    "        pg_advantages = (clipped_pg_rhos * (rewards + discounts * vs_t_plus_1 - values))\n",
    "\n",
    "        vs = tf.stop_gradient(vs)\n",
    "        pg_advantages = tf.stop_gradient(pg_advantages)\n",
    "\n",
    "        actor_loss = -tf.reduce_mean(target_action_log_probs * pg_advantages)\n",
    "\n",
    "        baseline_cost = 0.5\n",
    "        v_error = values - vs\n",
    "        critic_loss = baseline_cost * 0.5 * tf.reduce_mean(tf.square(v_error))\n",
    "\n",
    "        total_loss = actor_loss + critic_loss\n",
    "\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "def reinforcement_replay():\n",
    "    batch_size = 64\n",
    "    \n",
    "    memory_len = len(memory)\n",
    "    if len(memory) > batch_size:\n",
    "        start_index = random.randint(0, memory_len - batch_size)\n",
    "        minibatch = memory[start_index:start_index+batch_size]\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    states = np.zeros((batch_size, 64, 64, 4), dtype=np.float32)\n",
    "    actions = np.zeros(batch_size, dtype=np.int32)\n",
    "    policies = np.zeros((batch_size, num_actions), dtype=np.float32)\n",
    "    rewards = np.zeros(batch_size, dtype=np.float32)\n",
    "    dones = np.zeros(batch_size, dtype=np.bool)\n",
    "    memory_states = np.zeros((batch_size, 128), dtype=np.float32)\n",
    "    carry_states = np.zeros((batch_size, 128), dtype=np.float32)\n",
    "      \n",
    "    for i in range(len(minibatch)):\n",
    "        states[i] = minibatch[i][0]\n",
    "        actions[i] = minibatch[i][1]\n",
    "        policies[i] = minibatch[i][2]\n",
    "        rewards[i] = minibatch[i][3]\n",
    "        dones[i] = minibatch[i][4]\n",
    "        memory_states[i] = minibatch[i][5]\n",
    "        carry_states[i] = minibatch[i][6]\n",
    "\n",
    "    update(states, actions, policies, rewards, dones, memory_states, carry_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                 | 1/200000 [07:45<25873:23:18, 465.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_210452/3007550394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m#print(\"i: \", i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mreinforcement_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_210452/3007550394.py\u001b[0m in \u001b[0;36mreinforcement_train\u001b[0;34m(training_episode)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mreinforcement_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtotal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_210452/1363289515.py\u001b[0m in \u001b[0;36mreinforcement_replay\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mcarry_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcarry_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def reinforcement_train(training_episode):\n",
    "    total_reward, done, SAVING = 0, False, ''\n",
    "    obs = env.reset()\n",
    "    \n",
    "    state_list, action_list, policy_list, reward_list, done_list = [], [], [], [], []\n",
    "    \n",
    "    memory_state = tf.zeros([1,128], dtype=tf.dtypes.float32)\n",
    "    carry_state = tf.zeros([1,128], dtype=tf.dtypes.float32)\n",
    "    \n",
    "    total_step = 0\n",
    "    while True:\n",
    "        render(obs['pov'])\n",
    "        \n",
    "        pov_array = obs['pov'] / 255.0\n",
    "        \n",
    "        compassAngle_array = obs['compass']['angle'] / 360.0\n",
    "        compassAngle_array = np.ones((64,64,1)) * compassAngle_array\n",
    "        \n",
    "        state_array = np.concatenate((pov_array, compassAngle_array), 2)\n",
    "        state_array = np.expand_dims(state_array, 0)\n",
    "        \n",
    "        prediction = model(state_array, memory_state, carry_state, training=False)\n",
    "        act_pi = prediction[0]\n",
    "        next_memory_state = prediction[2]\n",
    "        next_carry_state = prediction[3]\n",
    "        \n",
    "        action_index = tf.random.categorical(act_pi, 1)\n",
    "        action_index = int(action_index)\n",
    "        \n",
    "        action = env.action_space.noop()\n",
    "        if (action_index == 0):\n",
    "            action['camera'] = [0, -2]\n",
    "        elif (action_index == 1):\n",
    "            action['camera'] = [0, 2]\n",
    "        elif (action_index == 2):\n",
    "            action['forward'] = 1\n",
    "            action['jump'] = 1\n",
    "            action['attack'] = 1\n",
    "            \n",
    "        obs_1, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        experience = state_array, action_index, act_pi, reward, done, memory_state, carry_state\n",
    "        memory.append((experience))\n",
    "        \n",
    "        obs = obs_1\n",
    "        memory_state = next_memory_state\n",
    "        carry_state = next_carry_state\n",
    "        if done:\n",
    "            print(\"total_reward: \", total_reward)\n",
    "            \n",
    "            with writer.as_default():\n",
    "                tf.summary.scalar(\"total_reward\", total_reward, step=training_episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            break\n",
    "        \n",
    "        if total_step % 10 == 0:\n",
    "            # train model\n",
    "            reinforcement_replay()\n",
    "\n",
    "        total_step += 1\n",
    "            \n",
    "    state_list, action_list, policy_list, reward_list, done_list = [], [], [], [], []\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "\n",
    "            \n",
    "max_episodes = 200000\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "  for i in t:\n",
    "    #print(\"i: \", i)\n",
    "    reinforcement_train(i)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        model.save_weights(workspace_path + '/model/' + str(i))\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import gym\n",
    "import minerl\n",
    "\n",
    "model.load_weights(workspace_path + \"/model/supervised_model_12000\")\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('MineRLNavigateDense-v0')\n",
    "\n",
    "seed = 980\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "reward_sum = 0\n",
    "for i_episode in range(0, 10000):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    pov_state = observation['pov'] / 255.0\n",
    "    compassAngle = observation['compass']['angle'] / 360.0\n",
    "    compassAngle_state = np.ones((64,64,1)) * compassAngle\n",
    "        \n",
    "    state = np.concatenate((pov_state, compassAngle_state), 2)\n",
    "    state = tf.constant(state, dtype=tf.float32)\n",
    "    \n",
    "    memory_state = tf.zeros([1,128], dtype=np.float32)\n",
    "    carry_state = tf.zeros([1,128], dtype=np.float32)\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        action_probs, _, memory_state, carry_state = model(state, memory_state, carry_state)\n",
    "        \n",
    "        action_dist = tfd.Categorical(probs=action_probs)\n",
    "        action_index = int(action_dist.sample()[0])\n",
    "        \n",
    "        action = env.action_space.noop()\n",
    "        if (action_index == 0):\n",
    "            action['camera'] = [0, -5]\n",
    "        elif (action_index == 1):\n",
    "            action['camera'] = [0, 5]\n",
    "        elif (action_index == 2):\n",
    "            action['forward'] = 1\n",
    "        elif (action_index == 3):\n",
    "            action['jump'] = 1\n",
    "            \n",
    "        observation_1, reward, done, info = env.step(action)\n",
    "        render(observation_1['pov'])\n",
    "        \n",
    "        pov_next_state = observation_1['pov'] / 255.0\n",
    "        compassAngle = observation_1['compass']['angle'] / 360.0\n",
    "        compassAngle_next_state = np.ones((64,64,1)) * compassAngle\n",
    "        \n",
    "        next_state = np.concatenate((pov_next_state, compassAngle_next_state), 2)\n",
    "        next_state = tf.constant(next_state, dtype=tf.float32)\n",
    "        \n",
    "        reward_sum += reward\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"Total reward: {:.2f},  Total step: {:.2f}\".format(reward_sum, step))\n",
    "            step = 0\n",
    "            reward_sum = 0  \n",
    "            #observation = env.reset()\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
